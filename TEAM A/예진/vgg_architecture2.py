# -*- coding: utf-8 -*-
"""vgg_architecture2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17z8JHcSQZb427D1bZlF9kUhG_R2PCnCq
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

# 1. VGGNet 모델 정의 (최적화)

def conv_2_block(in_dim, out_dim):
    return nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2)
    )

def conv_3_block(in_dim, out_dim):
    return nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2)
    )

class VGG(nn.Module):
    def __init__(self, base_dim, num_classes=10):
        super(VGG, self).__init__()
        self.feature = nn.Sequential(
            conv_2_block(3, base_dim),  # 64
            conv_2_block(base_dim, 2 * base_dim),  # 128
            conv_3_block(2 * base_dim, 4 * base_dim),  # 256
            conv_3_block(4 * base_dim, 8 * base_dim),  # 512
            conv_3_block(8 * base_dim, 8 * base_dim),  # 512
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(8 * base_dim * 1 * 1, 4096),  # CIFAR-10 (32x32) 맞춰서 수정
            nn.ReLU(True),
            nn.Dropout(0.5),
            nn.Linear(4096, 1000),
            nn.ReLU(True),
            nn.Dropout(0.5),
            nn.Linear(1000, num_classes),
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc_layer(x)
        return x


# 2. 데이터 로드 (최적화)

batch_size = 256  # 증가하여 학습 속도 향상
learning_rate = 0.001  # 조금 더 빠르게 학습
num_epoch = 5  # 10 → 5 (빠른 확인용)

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # 데이터 증강 추가
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

cifar10_train = datasets.CIFAR10(root="./data", train=True, transform=transform, download=True)
cifar10_test = datasets.CIFAR10(root="./data", train=False, transform=transform, download=True)

train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, num_workers=4)
test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=4)


# 3. 모델 설정 및 학습 (최적화)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VGG(base_dim=64).to(device)

loss_func = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate)  # Adam → AdamW로 변경
scaler = torch.cuda.amp.GradScaler()  # Mixed Precision 사용

loss_arr = []
for epoch in range(num_epoch):
    total_loss = 0
    model.train()
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epoch}"):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        with torch.cuda.amp.autocast():  # 반정밀도 연산으로 속도 증가
            outputs = model(images)
            loss = loss_func(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    loss_arr.append(avg_loss)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")


# 4. 학습 손실 시각화

plt.plot(loss_arr)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.show()


# 5. 모델 평가 (테스트 정확도)

correct = 0
total = 0
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")



"""# 경량 VGGNet로 모델 변경

합성곱 필터 수 / 파라미터 수 줄어들어 계산량 감소
"""

def conv_block(in_dim, out_dim):
    return nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2)
    )

class LightVGG(nn.Module): # 경량 VGGNet 모델
    def __init__(self, base_dim=32, num_classes=10):
        super(LightVGG, self).__init__()
        self.feature = nn.Sequential(
            conv_block(3, base_dim),  # 32
            conv_block(base_dim, base_dim * 2),  # 64
            conv_block(base_dim * 2, base_dim * 4),  # 128
            conv_block(base_dim * 4, base_dim * 8),  # 256
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim * 8 * 2 * 2, 512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, num_classes),
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layer(x)
        return x


batch_size = 512
learning_rate = 0.01
num_epoch = 5

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

cifar10_train = datasets.CIFAR10(root="./data", train=True, transform=transform, download=True)
cifar10_test = datasets.CIFAR10(root="./data", train=False, transform=transform, download=True)

train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, num_workers=4)
test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=4)



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LightVGG(base_dim=32).to(device)

loss_func = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epoch)  # 학습률 조정

loss_arr = []
for epoch in range(num_epoch):
    total_loss = 0
    model.train()
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epoch}"):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_func(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    loss_arr.append(avg_loss)
    scheduler.step()

    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")



plt.plot(loss_arr)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.show()



correct = 0
total = 0
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")



"""# VGG_G: FC층에 BatchNorm 추가

BatchNorm: 입력값 정규화, 데이터 분포 변하는 현상 감소, 과적합 방지
"""

def conv_2_block(in_dim, out_dim):
    return nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2)
    )

def conv_3_block(in_dim, out_dim):
    return nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2)
    )

class VGG_G(nn.Module):
    def __init__(self, base_dim=64, num_classes=10):
        super(VGG_G, self).__init__()
        self.feature = nn.Sequential(
            conv_2_block(3, base_dim),
            conv_2_block(base_dim, base_dim * 2),
            conv_3_block(base_dim * 2, base_dim * 4),
            conv_3_block(base_dim * 4, base_dim * 8),
            conv_3_block(base_dim * 8, base_dim * 8)
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim * 8 * 1 * 1, 4096),
            nn.BatchNorm1d(4096),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(4096, 1000),
            nn.BatchNorm1d(1000),
            nn.ReLU(True),
            nn.Dropout(),
            nn.Linear(1000, num_classes)
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layer(x)
        return x

batch_size = 100
learning_rate = 0.0002
num_epoch = 5

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)
train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VGG_G().to(device)
loss_func = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

loss_arr = []
for epoch in range(num_epoch):
    total_loss = 0
    model.train()
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epoch}"):
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_func(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    loss_arr.append(avg_loss)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

plt.plot(loss_arr)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("VGG_G Training Loss")
plt.show()

model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")

"""# VGG_H: Dropout 비율 증가 (0.5 → 0.7)

더 많은 뉴런이 랜덤하게 제거됨 → 다양한 특징 고르게 학습
"""

def conv_2_block(in_dim, out_dim):
    return nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2)
    )

def conv_3_block(in_dim, out_dim):
    return nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(2, 2)
    )

class VGG_H(nn.Module):
    def __init__(self, base_dim=64, num_classes=10):
        super(VGG_H, self).__init__()
        self.feature = nn.Sequential(
            conv_2_block(3, base_dim),
            conv_2_block(base_dim, base_dim * 2),
            conv_3_block(base_dim * 2, base_dim * 4),
            conv_3_block(base_dim * 4, base_dim * 8),
            conv_3_block(base_dim * 8, base_dim * 8)
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim * 8 * 1 * 1, 4096),
            nn.ReLU(True),
            nn.Dropout(p=0.7),
            nn.Linear(4096, 1000),
            nn.ReLU(True),
            nn.Dropout(p=0.7),
            nn.Linear(1000, num_classes)
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layer(x)
        return x

batch_size = 100
learning_rate = 0.0002
num_epoch = 5

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)
train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VGG_H().to(device)
loss_func = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

loss_arr = []
for epoch in range(num_epoch):
    total_loss = 0
    model.train()
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epoch}"):
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_func(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    loss_arr.append(avg_loss)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

plt.plot(loss_arr)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("VGG_H Training Loss")
plt.show()

model.eval()
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")



"""# 초소형 CNN 모델 변경"""

class SmallCNN(nn.Module):
    def __init__(self, base_dim=16, num_classes=10):
        super(SmallCNN, self).__init__()
        self.feature = nn.Sequential(
            nn.Conv2d(3, base_dim, kernel_size=3, padding=1),
            nn.SiLU(),  # ReLU 대신 SiLU 사용
            nn.Conv2d(base_dim, base_dim * 2, kernel_size=3, padding=1),
            nn.SiLU(),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(base_dim * 2, base_dim * 4, kernel_size=3, padding=1),
            nn.SiLU(),
            nn.Conv2d(base_dim * 4, base_dim * 8, kernel_size=3, padding=1),
            nn.SiLU(),
            nn.MaxPool2d(2, 2),
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim * 8 * 8 * 8, 256),
            nn.SiLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes),
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc_layer(x)
        return x


batch_size = 1024  # 한 번에 많은 데이터 학습
learning_rate = 0.003  # OneCycleLR과 함께 사용
num_epoch = 5  # 5회 학습

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

cifar10_train = datasets.CIFAR10(root="./data", train=True, transform=transform, download=True)
cifar10_test = datasets.CIFAR10(root="./data", train=False, transform=transform, download=True)

train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, num_workers=4)
test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=4)



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SmallCNN(base_dim=16).to(device)

loss_func = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=num_epoch)

loss_arr = []
for epoch in range(num_epoch):
    total_loss = 0
    model.train()
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epoch}"):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_func(outputs, labels)
        loss.backward()
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    loss_arr.append(avg_loss)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")


plt.plot(loss_arr)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.show()


correct = 0
total = 0
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")

# 1. 초경량 CNN 모델 정의 (EfficientNet 스타일)

class DepthwiseSeparableConv(nn.Module):
    """Depthwise Separable Convolution 적용 (EfficientNet 스타일)"""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super(DepthwiseSeparableConv, self).__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=in_channels)
        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.activation = nn.Mish()

    def forward(self, x):
        x = self.depthwise(x)
        x = self.activation(x)
        x = self.pointwise(x)
        x = self.activation(x)
        return x

class EfficientCNN(nn.Module):
    def __init__(self, base_dim=8, num_classes=10):
        super(EfficientCNN, self).__init__()
        self.feature = nn.Sequential(
            DepthwiseSeparableConv(3, base_dim),
            DepthwiseSeparableConv(base_dim, base_dim * 2),
            nn.MaxPool2d(2, 2),

            DepthwiseSeparableConv(base_dim * 2, base_dim * 4),
            DepthwiseSeparableConv(base_dim * 4, base_dim * 8),
            nn.MaxPool2d(2, 2),
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim * 8 * 8 * 8, 128),
            nn.Mish(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc_layer(x)
        return x

# ========================
# 2. 데이터 로드
# ========================
batch_size = 2048  # 최대 배치 크기 적용
learning_rate = 0.05  # Cosine Annealing 적용
num_epoch = 3  # 짧은 시간 내 수렴

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

cifar10_train = datasets.CIFAR10(root="./data", train=True, transform=transform, download=True)
cifar10_test = datasets.CIFAR10(root="./data", train=False, transform=transform, download=True)

train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, num_workers=4)
test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=4)

# ========================
# 3. 모델 설정 및 학습
# ========================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = EfficientCNN(base_dim=8).to(device)

loss_func = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epoch)

loss_arr = []
for epoch in range(num_epoch):
    total_loss = 0
    model.train()
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epoch}"):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_func(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    scheduler.step()
    avg_loss = total_loss / len(train_loader)
    loss_arr.append(avg_loss)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

# ========================
# 4. 학습 손실 시각화
# ========================
plt.plot(loss_arr)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.show()

# ========================
# 5. 모델 평가 (테스트 정확도)
# ========================
correct = 0
total = 0
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

# ========================
# 1. MobileNetV2 스타일의 Inverted Residual Block 정의
# ========================
class InvertedResidualBlock(nn.Module):
    """MobileNetV2 스타일의 Inverted Residual Block"""
    def __init__(self, in_channels, out_channels, expansion=6, stride=1):
        super(InvertedResidualBlock, self).__init__()
        hidden_dim = in_channels * expansion
        self.use_residual = stride == 1 and in_channels == out_channels

        self.block = nn.Sequential(
            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),
            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.ReLU6(inplace=True),
            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False),
            nn.BatchNorm2d(out_channels),
        )

    def forward(self, x):
        if self.use_residual:
            return x + self.block(x)
        else:
            return self.block(x)

# ========================
# 2. MobileNetV2 기반 경량 CNN 모델 정의
# ========================
class MobileNetV2Mini(nn.Module):
    def __init__(self, num_classes=10):
        super(MobileNetV2Mini, self).__init__()
        self.initial_conv = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU6(inplace=True)
        )

        self.layers = nn.Sequential(
            InvertedResidualBlock(32, 16, expansion=1, stride=1),
            InvertedResidualBlock(16, 24, expansion=6, stride=2),
            InvertedResidualBlock(24, 32, expansion=6, stride=2),
            InvertedResidualBlock(32, 64, expansion=6, stride=2),
            InvertedResidualBlock(64, 96, expansion=6, stride=1),
            InvertedResidualBlock(96, 160, expansion=6, stride=2),
            InvertedResidualBlock(160, 320, expansion=6, stride=1),
        )

        self.final_conv = nn.Sequential(
            nn.Conv2d(320, 1280, kernel_size=1, bias=False),
            nn.BatchNorm2d(1280),
            nn.ReLU6(inplace=True)
        )

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(1280, num_classes)

    def forward(self, x):
        x = self.initial_conv(x)
        x = self.layers(x)
        x = self.final_conv(x)
        x = self.avg_pool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

# ========================
# 3. 데이터 로드
# ========================
batch_size = 512  # 빠른 학습을 위해 적절한 크기 설정
learning_rate = 0.005  # AdamW와 함께 사용
num_epoch = 8  # 가벼운 모델이므로 약간 증가

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

cifar10_train = datasets.CIFAR10(root="./data", train=True, transform=transform, download=True)
cifar10_test = datasets.CIFAR10(root="./data", train=False, transform=transform, download=True)

train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, num_workers=4)
test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=4)

# ========================
# 4. 모델 설정 및 학습
# ========================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MobileNetV2Mini(num_classes=10).to(device)

loss_func = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epoch)

loss_arr = []
for epoch in range(num_epoch):
    total_loss = 0
    model.train()
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epoch}"):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_func(outputs, labels)
        loss.backward()
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    loss_arr.append(avg_loss)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

# ========================
# 5. 학습 손실 시각화
# ========================
plt.plot(loss_arr)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.show()

# ========================
# 6. 모델 평가 (테스트 정확도)
# ========================
correct = 0
total = 0
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

# ========================
# 1. ResNet 스타일의 Basic Residual Block 정의
# ========================
class BasicBlock(nn.Module):
    """ResNet의 기본 블록"""
    def __init__(self, in_channels, out_channels, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += self.shortcut(x)  # Skip Connection
        out = self.relu(out)
        return out

# ========================
# 2. ResNet 기반 경량 CNN 모델 정의
# ========================
class ResNetMini(nn.Module):
    def __init__(self, num_classes=10):
        super(ResNetMini, self).__init__()
        self.init_conv = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )

        self.layer1 = self._make_layer(32, 64, stride=2)
        self.layer2 = self._make_layer(64, 128, stride=2)
        self.layer3 = self._make_layer(128, 256, stride=2)

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(256, num_classes)

    def _make_layer(self, in_channels, out_channels, stride):
        return nn.Sequential(
            BasicBlock(in_channels, out_channels, stride),
            BasicBlock(out_channels, out_channels, 1)
        )

    def forward(self, x):
        x = self.init_conv(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.avg_pool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

# ========================
# 3. 데이터 로드
# ========================
batch_size = 256  # 학습 속도와 성능의 균형 유지
learning_rate = 0.002  # Adam과 함께 사용
num_epoch = 6  # ResNet 스타일은 적은 Epoch에서도 효과적

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

cifar10_train = datasets.CIFAR10(root="./data", train=True, transform=transform, download=True)
cifar10_test = datasets.CIFAR10(root="./data", train=False, transform=transform, download=True)

train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True, num_workers=4)
test_loader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=4)

# ========================
# 4. 모델 설정 및 학습
# ========================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ResNetMini(num_classes=10).to(device)

loss_func = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)

loss_arr = []
for epoch in range(num_epoch):
    total_loss = 0
    model.train()
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epoch}"):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = loss_func(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    scheduler.step()
    avg_loss = total_loss / len(train_loader)
    loss_arr.append(avg_loss)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

# ========================
# 5. 학습 손실 시각화
# ========================
plt.plot(loss_arr)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.show()

# ========================
# 6. 모델 평가 (테스트 정확도)
# ========================
correct = 0
total = 0
model.eval()

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Test Accuracy: {accuracy:.2f}%")