import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from torch.utils.tensorboard import SummaryWriter

from dataset import MoNuSegDataset
from model import AttentionUNet  # UNet ëª¨ë¸ì´ model.pyì— ìˆë‹¤ê³  ê°€ì •
from utils import save_checkpoint, load_checkpoint, fn_tonumpy, fn_denorm, fn_class, DiceLoss  # ìœ í‹¸ í•¨ìˆ˜ë“¤

import matplotlib.pyplot as plt

# ----- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • -----
batch_size = 4
num_epoch = 200
learning_rate = 1e-4
log_dir = './logs'
ckpt_dir = './checkpoints'
result_dir = './results'
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ----- ë°ì´í„° ë¡œë” ì¤€ë¹„ -----
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])

train_dataset = MoNuSegDataset(
    image_dir='./datasets/training/Tissue Images',
    annotation_dir='./datasets/training/Annotations',
    transform=transform
)


# ë°ì´í„°ì…‹ì„ validation setìœ¼ë¡œ ì¼ë¶€ ë‚˜ëˆ„ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì„œ ë”°ë¡œ ë¶„ë¦¬ ê°€ëŠ¥
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # ê°„ë‹¨íˆ train ë°ì´í„° ì¬ì‚¬ìš© (ì •ì‹ val set ìˆìœ¼ë©´ êµì²´)

num_batch_train = len(train_loader)
num_batch_val = len(val_loader)

# ----- ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € -----
net = AttentionUNet().to(device)

bce_loss = nn.BCEWithLogitsLoss().to(device)
dice_loss = DiceLoss().to(device)
def fn_loss(output, label):
    return bce_loss(output, label) + dice_loss(output, label)  # BEC + Dice ë¡œ êµ¬ì„± 

optimizer = optim.Adam(net.parameters(), lr=learning_rate)

# ----- Tensorboard SummaryWriter -----
writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))
writer_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))

# ----- í•™ìŠµ/ê²€ì¦ ì‹œì‘ -----
st_epoch = 0
train_continue = 'on'  # ì €ì¥ëœ ëª¨ë¸ ì´ì–´ì„œ í•™ìŠµí• ì§€ ì—¬ë¶€ ("on"ì´ë©´ load). 60ì—í­ë¶€í„° ì´ì–´ í•™ìŠµ 

if train_continue == 'on':
    net, optimizer, st_epoch = load_checkpoint(ckpt_dir=ckpt_dir, net=net, optimizer=optimizer)

for epoch in range(st_epoch + 1, num_epoch + 1):
    net.train()
    loss_arr = []

    for batch, (input, label) in enumerate(train_loader, 1):
        input, label = input.to(device), label.to(device)

        output = net(input)
        loss = fn_loss(output, label)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        loss_arr.append(loss.item())

        print(f"TRAIN: EPOCH {epoch:04d}/{num_epoch:04d} | BATCH {batch:04d}/{num_batch_train:04d} | LOSS {np.mean(loss_arr):.4f}")

        # TensorBoard ë¡œê·¸ ì €ì¥
        step = num_batch_train * (epoch - 1) + batch

        writer_train.add_image('input', fn_tonumpy(fn_denorm(input)), step, dataformats='NHWC')
        writer_train.add_image('label', fn_tonumpy(label), step, dataformats='NHWC')
        writer_train.add_image('output', fn_tonumpy(fn_class(output)), step, dataformats='NHWC')

    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)

    # ----- Validation -----
    with torch.no_grad():
        net.eval()
        loss_arr = []

        for batch, (input, label) in enumerate(val_loader, 1):
            input, label = input.to(device), label.to(device)

            output = net(input)
            loss = fn_loss(output, label)

            loss_arr.append(loss.item())

            print(f"VALID: EPOCH {epoch:04d}/{num_epoch:04d} | BATCH {batch:04d}/{num_batch_val:04d} | LOSS {np.mean(loss_arr):.4f}")

            step = num_batch_val * (epoch - 1) + batch

            writer_val.add_image('input', fn_tonumpy(fn_denorm(input)), step, dataformats='NHWC')
            writer_val.add_image('label', fn_tonumpy(label), step, dataformats='NHWC')
            writer_val.add_image('output', fn_tonumpy(fn_class(output)), step, dataformats='NHWC')

        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)

    # ----- ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì €ì¥ -----
    if epoch % 20 == 0:
        save_checkpoint(ckpt_dir=ckpt_dir, net=net, optimizer=optimizer, epoch=epoch)

# ìµœì¢… ì¢…ë£Œ
writer_train.close()
writer_val.close()

print("Training Finished Successfully ğŸš€")
