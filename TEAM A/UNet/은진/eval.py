# ğŸ“ Step 5: eval.py ğŸ¤—
# í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥

import os
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from torchvision import transforms

from model import UNet
from dataset import *
from util import *

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
lr = 1e-3
batch_size = 4
num_epoch = 100

# ê²½ë¡œ ì„¤ì • (Colab ê¸°ë°˜ ê²½ë¡œ ì˜ˆì‹œ)
data_dir = './datasets'
ckpt_dir = './checkpoint'
log_dir = './log'
result_dir = './results'

# ê²°ê³¼ ì €ì¥ í´ë” ì—†ìœ¼ë©´ ìƒì„±
if not os.path.exists(result_dir):
    os.makedirs(os.path.join(result_dir, 'png'))
    os.makedirs(os.path.join(result_dir, 'numpy'))

# GPU ì‚¬ìš© ì—¬ë¶€ ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ë„¤íŠ¸ì›Œí¬, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜
net = UNet().to(device)
fn_loss = nn.BCEWithLogitsLoss().to(device)
optim = torch.optim.Adam(net.parameters(), lr=lr)

# Transform ì •ì˜ (í…ŒìŠ¤íŠ¸ì—ëŠ” augmentation ì œì™¸)
transform = transforms.Compose([
    Normalization(mean=0.5, std=0.5),
    ToTensor()
])

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë”©
dataset_test = Dataset(data_dir=os.path.join(data_dir, 'test'), transform=transform)
loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=8)

# ë¶€ìˆ˜ í•¨ìˆ˜ë“¤ ì •ì˜
to_numpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)
denorm = lambda x, mean, std: (x * std) + mean
binarize = lambda x: 1.0 * (x > 0.5)

# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)

# í…ŒìŠ¤íŠ¸ ì‹œì‘
with torch.no_grad():
    net.eval()
    loss_arr = []

    for batch, data in enumerate(loader_test, 1):
        label = data['label'].to(device)
        input = data['input'].to(device)
        output = net(input)

        loss = fn_loss(output, label)
        loss_arr += [loss.item()]

        print("TEST: BATCH %04d / %04d | LOSS %.4f" %
              (batch, np.ceil(len(dataset_test)/batch_size), np.mean(loss_arr)))

        label = to_numpy(label)
        input = to_numpy(denorm(input, mean=0.5, std=0.5))
        output = to_numpy(binarize(output))

        for j in range(label.shape[0]):
            id = int(np.ceil(len(dataset_test)/batch_size)) * (batch - 1) + j

            # PNG ì €ì¥
            plt.imsave(os.path.join(result_dir, 'png', f'label_{id:04d}.png'), label[j].squeeze(), cmap='gray')
            plt.imsave(os.path.join(result_dir, 'png', f'input_{id:04d}.png'), input[j].squeeze(), cmap='gray')
            plt.imsave(os.path.join(result_dir, 'png', f'output_{id:04d}.png'), output[j].squeeze(), cmap='gray')

            # NumPy ì €ì¥
            np.save(os.path.join(result_dir, 'numpy', f'label_{id:04d}.npy'), label[j].squeeze())
            np.save(os.path.join(result_dir, 'numpy', f'input_{id:04d}.npy'), input[j].squeeze())
            np.save(os.path.join(result_dir, 'numpy', f'output_{id:04d}.npy'), output[j].squeeze())

# ì „ì²´ í‰ê·  ì†ì‹¤ ì¶œë ¥
print("AVERAGE TEST: BATCH %04d / %04d | LOSS %.4f" %
      (batch, np.ceil(len(dataset_test)/batch_size), np.mean(loss_arr)))