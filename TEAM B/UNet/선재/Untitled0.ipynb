{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMJPzFD45lv3NkvZTwMCZuS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQhMiQz-69h-","executionInfo":{"status":"ok","timestamp":1743520768533,"user_tz":-540,"elapsed":19459,"user":{"displayName":"‍홍선재(학부생-소프트웨어전공)","userId":"17464694903666453963"}},"outputId":"5d5bb8ae-124e-4793-b1d3-c36ed56cdfc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# 해당 디렉토리로 이동\n","%cd /content/drive/MyDrive/YouTube/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dFZZ2q9I6_25","executionInfo":{"status":"ok","timestamp":1743520768553,"user_tz":-540,"elapsed":16,"user":{"displayName":"‍홍선재(학부생-소프트웨어전공)","userId":"17464694903666453963"}},"outputId":"27701c45-43cf-419d-b2bd-3ed7c9e4ef19"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/YouTube\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/hanyoseob/youtube-cnn-002-pytorch-unet/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6YAFI5z7P7M","executionInfo":{"status":"ok","timestamp":1743520604296,"user_tz":-540,"elapsed":3215,"user":{"displayName":"‍홍선재(학부생-소프트웨어전공)","userId":"17464694903666453963"}},"outputId":"5eaae035-6954-4a19-f939-1d2b77808be1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'youtube-cnn-002-pytorch-unet'...\n","remote: Enumerating objects: 65, done.\u001b[K\n","remote: Counting objects: 100% (65/65), done.\u001b[K\n","remote: Compressing objects: 100% (48/48), done.\u001b[K\n","remote: Total 65 (delta 29), reused 52 (delta 16), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (65/65), 14.27 MiB | 11.69 MiB/s, done.\n","Resolving deltas: 100% (29/29), done.\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/YouTube/youtube-cnn-002-pytorch-unet/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrYpxRq07RLY","executionInfo":{"status":"ok","timestamp":1743520772671,"user_tz":-540,"elapsed":9,"user":{"displayName":"‍홍선재(학부생-소프트웨어전공)","userId":"17464694903666453963"}},"outputId":"ae9eaf2e-0dd5-4991-aa6a-b832645ccda9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/YouTube/youtube-cnn-002-pytorch-unet\n"]}]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"Gmelu_8-7Ttz","executionInfo":{"status":"ok","timestamp":1743520773682,"user_tz":-540,"elapsed":261,"user":{"displayName":"‍홍선재(학부생-소프트웨어전공)","userId":"17464694903666453963"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!python 'data_read.py'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1Bm5iAG7Uqn","executionInfo":{"status":"ok","timestamp":1743520798626,"user_tz":-540,"elapsed":24035,"user":{"displayName":"‍홍선재(학부생-소프트웨어전공)","userId":"17464694903666453963"}},"outputId":"b40391e8-f5e9-45f8-e375-3165dad77596"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Figure(640x480)\n"]}]},{"cell_type":"code","source":["!python3 'train.py'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZBvNfxzn7Vwm","executionInfo":{"status":"ok","timestamp":1743522038216,"user_tz":-540,"elapsed":1239592,"user":{"displayName":"‍홍선재(학부생-소프트웨어전공)","userId":"17464694903666453963"}},"outputId":"ca2a1892-8413-40ec-a4c6-67945c874b37"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-04-01 15:20:03.504730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1743520803.556061     733 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1743520803.570878     733 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-04-01 15:20:03.608272: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","learning rate: 1.0000e-03\n","batch size: 4\n","number of epoch: 100\n","data dir: ./datasets\n","ckpt dir: ./checkpoint\n","log dir: ./log\n","result dir: ./result\n","mode: train\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0001 / 0006 | LOSS 0.8764\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0002 / 0006 | LOSS 0.7929\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0003 / 0006 | LOSS 0.7529\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0004 / 0006 | LOSS 0.7195\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0005 / 0006 | LOSS 0.6898\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0006 / 0006 | LOSS 0.6645\n","VALID: EPOCH 0001 / 0100 | BATCH 0001 / 0001 | LOSS 0.6610\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0001 / 0006 | LOSS 0.5376\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0002 / 0006 | LOSS 0.5104\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0003 / 0006 | LOSS 0.4960\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0004 / 0006 | LOSS 0.4857\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0005 / 0006 | LOSS 0.4814\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0006 / 0006 | LOSS 0.4754\n","VALID: EPOCH 0002 / 0100 | BATCH 0001 / 0001 | LOSS 0.5946\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0001 / 0006 | LOSS 0.4457\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0002 / 0006 | LOSS 0.4333\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0003 / 0006 | LOSS 0.4289\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0004 / 0006 | LOSS 0.4258\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0005 / 0006 | LOSS 0.4218\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0006 / 0006 | LOSS 0.4157\n","VALID: EPOCH 0003 / 0100 | BATCH 0001 / 0001 | LOSS 0.4394\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0001 / 0006 | LOSS 0.4316\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0002 / 0006 | LOSS 0.4143\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0003 / 0006 | LOSS 0.4028\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0004 / 0006 | LOSS 0.4003\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0005 / 0006 | LOSS 0.3956\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0006 / 0006 | LOSS 0.3939\n","VALID: EPOCH 0004 / 0100 | BATCH 0001 / 0001 | LOSS 0.4766\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0001 / 0006 | LOSS 0.3721\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0002 / 0006 | LOSS 0.3704\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0003 / 0006 | LOSS 0.3716\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0004 / 0006 | LOSS 0.3676\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0005 / 0006 | LOSS 0.3674\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0006 / 0006 | LOSS 0.3670\n","VALID: EPOCH 0005 / 0100 | BATCH 0001 / 0001 | LOSS 0.4012\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0001 / 0006 | LOSS 0.3457\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0002 / 0006 | LOSS 0.3467\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0003 / 0006 | LOSS 0.3441\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0004 / 0006 | LOSS 0.3415\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0005 / 0006 | LOSS 0.3432\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0006 / 0006 | LOSS 0.3430\n","VALID: EPOCH 0006 / 0100 | BATCH 0001 / 0001 | LOSS 0.3996\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0001 / 0006 | LOSS 0.3245\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0002 / 0006 | LOSS 0.3219\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0003 / 0006 | LOSS 0.3281\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0004 / 0006 | LOSS 0.3301\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0005 / 0006 | LOSS 0.3289\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0006 / 0006 | LOSS 0.3264\n","VALID: EPOCH 0007 / 0100 | BATCH 0001 / 0001 | LOSS 0.3710\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0001 / 0006 | LOSS 0.3154\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0002 / 0006 | LOSS 0.3147\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0003 / 0006 | LOSS 0.3162\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0004 / 0006 | LOSS 0.3116\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0005 / 0006 | LOSS 0.3098\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0006 / 0006 | LOSS 0.3077\n","VALID: EPOCH 0008 / 0100 | BATCH 0001 / 0001 | LOSS 0.3548\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0001 / 0006 | LOSS 0.3001\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0002 / 0006 | LOSS 0.2944\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0003 / 0006 | LOSS 0.2928\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0004 / 0006 | LOSS 0.2952\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0005 / 0006 | LOSS 0.2939\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0006 / 0006 | LOSS 0.2921\n","VALID: EPOCH 0009 / 0100 | BATCH 0001 / 0001 | LOSS 0.3050\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0001 / 0006 | LOSS 0.2810\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0002 / 0006 | LOSS 0.2854\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0003 / 0006 | LOSS 0.2837\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0004 / 0006 | LOSS 0.2807\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0005 / 0006 | LOSS 0.2796\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0006 / 0006 | LOSS 0.2787\n","VALID: EPOCH 0010 / 0100 | BATCH 0001 / 0001 | LOSS 0.3092\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0001 / 0006 | LOSS 0.2771\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0002 / 0006 | LOSS 0.2692\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0003 / 0006 | LOSS 0.2747\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0004 / 0006 | LOSS 0.2727\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0005 / 0006 | LOSS 0.2701\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0006 / 0006 | LOSS 0.2704\n","VALID: EPOCH 0011 / 0100 | BATCH 0001 / 0001 | LOSS 0.2766\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0001 / 0006 | LOSS 0.2708\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0002 / 0006 | LOSS 0.2710\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0003 / 0006 | LOSS 0.2659\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0004 / 0006 | LOSS 0.2624\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0005 / 0006 | LOSS 0.2624\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0006 / 0006 | LOSS 0.2611\n","VALID: EPOCH 0012 / 0100 | BATCH 0001 / 0001 | LOSS 0.2845\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0001 / 0006 | LOSS 0.2584\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0002 / 0006 | LOSS 0.2554\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0003 / 0006 | LOSS 0.2521\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0004 / 0006 | LOSS 0.2512\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0005 / 0006 | LOSS 0.2548\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0006 / 0006 | LOSS 0.2543\n","VALID: EPOCH 0013 / 0100 | BATCH 0001 / 0001 | LOSS 0.2771\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0001 / 0006 | LOSS 0.2478\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0002 / 0006 | LOSS 0.2538\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0003 / 0006 | LOSS 0.2508\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0004 / 0006 | LOSS 0.2493\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0005 / 0006 | LOSS 0.2506\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0006 / 0006 | LOSS 0.2514\n","VALID: EPOCH 0014 / 0100 | BATCH 0001 / 0001 | LOSS 0.2615\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0001 / 0006 | LOSS 0.2454\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0002 / 0006 | LOSS 0.2456\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0003 / 0006 | LOSS 0.2495\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0004 / 0006 | LOSS 0.2449\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0005 / 0006 | LOSS 0.2433\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0006 / 0006 | LOSS 0.2423\n","VALID: EPOCH 0015 / 0100 | BATCH 0001 / 0001 | LOSS 0.2972\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0001 / 0006 | LOSS 0.2529\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0002 / 0006 | LOSS 0.2476\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0003 / 0006 | LOSS 0.2474\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0004 / 0006 | LOSS 0.2414\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0005 / 0006 | LOSS 0.2389\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0006 / 0006 | LOSS 0.2416\n","VALID: EPOCH 0016 / 0100 | BATCH 0001 / 0001 | LOSS 0.2281\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0001 / 0006 | LOSS 0.2308\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0002 / 0006 | LOSS 0.2360\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0003 / 0006 | LOSS 0.2323\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0004 / 0006 | LOSS 0.2298\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0005 / 0006 | LOSS 0.2306\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0006 / 0006 | LOSS 0.2295\n","VALID: EPOCH 0017 / 0100 | BATCH 0001 / 0001 | LOSS 0.2523\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0001 / 0006 | LOSS 0.2416\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0002 / 0006 | LOSS 0.2351\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0003 / 0006 | LOSS 0.2316\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0004 / 0006 | LOSS 0.2260\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0005 / 0006 | LOSS 0.2262\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0006 / 0006 | LOSS 0.2263\n","VALID: EPOCH 0018 / 0100 | BATCH 0001 / 0001 | LOSS 0.2273\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0001 / 0006 | LOSS 0.2250\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0002 / 0006 | LOSS 0.2285\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0003 / 0006 | LOSS 0.2242\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0004 / 0006 | LOSS 0.2288\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0005 / 0006 | LOSS 0.2250\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0006 / 0006 | LOSS 0.2227\n","VALID: EPOCH 0019 / 0100 | BATCH 0001 / 0001 | LOSS 0.2292\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0001 / 0006 | LOSS 0.2134\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0002 / 0006 | LOSS 0.2137\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0003 / 0006 | LOSS 0.2205\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0004 / 0006 | LOSS 0.2192\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0005 / 0006 | LOSS 0.2199\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0006 / 0006 | LOSS 0.2183\n","VALID: EPOCH 0020 / 0100 | BATCH 0001 / 0001 | LOSS 0.2130\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0001 / 0006 | LOSS 0.2216\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0002 / 0006 | LOSS 0.2133\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0003 / 0006 | LOSS 0.2135\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0004 / 0006 | LOSS 0.2169\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0005 / 0006 | LOSS 0.2171\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0006 / 0006 | LOSS 0.2159\n","VALID: EPOCH 0021 / 0100 | BATCH 0001 / 0001 | LOSS 0.2212\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0001 / 0006 | LOSS 0.2122\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0002 / 0006 | LOSS 0.2105\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0003 / 0006 | LOSS 0.2135\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0004 / 0006 | LOSS 0.2139\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0005 / 0006 | LOSS 0.2176\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0006 / 0006 | LOSS 0.2158\n","VALID: EPOCH 0022 / 0100 | BATCH 0001 / 0001 | LOSS 0.2134\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0001 / 0006 | LOSS 0.2017\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0002 / 0006 | LOSS 0.2113\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0003 / 0006 | LOSS 0.2123\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0004 / 0006 | LOSS 0.2141\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0005 / 0006 | LOSS 0.2130\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0006 / 0006 | LOSS 0.2126\n","VALID: EPOCH 0023 / 0100 | BATCH 0001 / 0001 | LOSS 0.2128\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0001 / 0006 | LOSS 0.2028\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0002 / 0006 | LOSS 0.2108\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0003 / 0006 | LOSS 0.2131\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0004 / 0006 | LOSS 0.2135\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0005 / 0006 | LOSS 0.2121\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0006 / 0006 | LOSS 0.2104\n","VALID: EPOCH 0024 / 0100 | BATCH 0001 / 0001 | LOSS 0.2119\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0001 / 0006 | LOSS 0.2128\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0002 / 0006 | LOSS 0.2090\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0003 / 0006 | LOSS 0.2091\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0004 / 0006 | LOSS 0.2120\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0005 / 0006 | LOSS 0.2111\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0006 / 0006 | LOSS 0.2080\n","VALID: EPOCH 0025 / 0100 | BATCH 0001 / 0001 | LOSS 0.1926\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0001 / 0006 | LOSS 0.2061\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0002 / 0006 | LOSS 0.2231\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0003 / 0006 | LOSS 0.2184\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0004 / 0006 | LOSS 0.2130\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0005 / 0006 | LOSS 0.2154\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0006 / 0006 | LOSS 0.2124\n","VALID: EPOCH 0026 / 0100 | BATCH 0001 / 0001 | LOSS 0.2346\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0001 / 0006 | LOSS 0.1981\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0002 / 0006 | LOSS 0.2062\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0003 / 0006 | LOSS 0.2034\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0004 / 0006 | LOSS 0.2058\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0005 / 0006 | LOSS 0.2072\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0006 / 0006 | LOSS 0.2061\n","VALID: EPOCH 0027 / 0100 | BATCH 0001 / 0001 | LOSS 0.1981\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0001 / 0006 | LOSS 0.1968\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0002 / 0006 | LOSS 0.1932\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0003 / 0006 | LOSS 0.1977\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0004 / 0006 | LOSS 0.1971\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0005 / 0006 | LOSS 0.1995\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0006 / 0006 | LOSS 0.2005\n","VALID: EPOCH 0028 / 0100 | BATCH 0001 / 0001 | LOSS 0.2045\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0001 / 0006 | LOSS 0.1931\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0002 / 0006 | LOSS 0.1964\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0003 / 0006 | LOSS 0.1970\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0004 / 0006 | LOSS 0.1998\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0005 / 0006 | LOSS 0.1977\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0006 / 0006 | LOSS 0.1972\n","VALID: EPOCH 0029 / 0100 | BATCH 0001 / 0001 | LOSS 0.1988\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0001 / 0006 | LOSS 0.1795\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0002 / 0006 | LOSS 0.1954\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0003 / 0006 | LOSS 0.2001\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0004 / 0006 | LOSS 0.2002\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0005 / 0006 | LOSS 0.1987\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0006 / 0006 | LOSS 0.1994\n","VALID: EPOCH 0030 / 0100 | BATCH 0001 / 0001 | LOSS 0.2016\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0001 / 0006 | LOSS 0.2082\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0002 / 0006 | LOSS 0.1994\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0003 / 0006 | LOSS 0.1927\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0004 / 0006 | LOSS 0.1918\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0005 / 0006 | LOSS 0.1940\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0006 / 0006 | LOSS 0.1967\n","VALID: EPOCH 0031 / 0100 | BATCH 0001 / 0001 | LOSS 0.1917\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0001 / 0006 | LOSS 0.1841\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0002 / 0006 | LOSS 0.1912\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0003 / 0006 | LOSS 0.1959\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0004 / 0006 | LOSS 0.1930\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0005 / 0006 | LOSS 0.1944\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0006 / 0006 | LOSS 0.1956\n","VALID: EPOCH 0032 / 0100 | BATCH 0001 / 0001 | LOSS 0.1967\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0001 / 0006 | LOSS 0.1868\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0002 / 0006 | LOSS 0.1889\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0003 / 0006 | LOSS 0.1921\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0004 / 0006 | LOSS 0.1930\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0005 / 0006 | LOSS 0.1926\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0006 / 0006 | LOSS 0.1938\n","VALID: EPOCH 0033 / 0100 | BATCH 0001 / 0001 | LOSS 0.2312\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0001 / 0006 | LOSS 0.1903\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0002 / 0006 | LOSS 0.1982\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0003 / 0006 | LOSS 0.1997\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0004 / 0006 | LOSS 0.1959\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0005 / 0006 | LOSS 0.1916\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0006 / 0006 | LOSS 0.1910\n","VALID: EPOCH 0034 / 0100 | BATCH 0001 / 0001 | LOSS 0.1950\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0001 / 0006 | LOSS 0.2033\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0002 / 0006 | LOSS 0.2032\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0003 / 0006 | LOSS 0.1962\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0004 / 0006 | LOSS 0.1971\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0005 / 0006 | LOSS 0.1968\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0006 / 0006 | LOSS 0.1951\n","VALID: EPOCH 0035 / 0100 | BATCH 0001 / 0001 | LOSS 0.1919\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0001 / 0006 | LOSS 0.2081\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0002 / 0006 | LOSS 0.1981\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0003 / 0006 | LOSS 0.1935\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0004 / 0006 | LOSS 0.1945\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0005 / 0006 | LOSS 0.1942\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0006 / 0006 | LOSS 0.1977\n","VALID: EPOCH 0036 / 0100 | BATCH 0001 / 0001 | LOSS 0.2157\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0001 / 0006 | LOSS 0.1885\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0002 / 0006 | LOSS 0.1988\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0003 / 0006 | LOSS 0.1950\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0004 / 0006 | LOSS 0.1928\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0005 / 0006 | LOSS 0.1954\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0006 / 0006 | LOSS 0.1954\n","VALID: EPOCH 0037 / 0100 | BATCH 0001 / 0001 | LOSS 0.1987\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0001 / 0006 | LOSS 0.1968\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0002 / 0006 | LOSS 0.1937\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0003 / 0006 | LOSS 0.1899\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0004 / 0006 | LOSS 0.1893\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0005 / 0006 | LOSS 0.1881\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0006 / 0006 | LOSS 0.1930\n","VALID: EPOCH 0038 / 0100 | BATCH 0001 / 0001 | LOSS 0.1903\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0001 / 0006 | LOSS 0.1760\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0002 / 0006 | LOSS 0.1787\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0003 / 0006 | LOSS 0.1816\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0004 / 0006 | LOSS 0.1854\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0005 / 0006 | LOSS 0.1890\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0006 / 0006 | LOSS 0.1904\n","VALID: EPOCH 0039 / 0100 | BATCH 0001 / 0001 | LOSS 0.1847\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0001 / 0006 | LOSS 0.1838\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0002 / 0006 | LOSS 0.1910\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0003 / 0006 | LOSS 0.1879\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0004 / 0006 | LOSS 0.1902\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0005 / 0006 | LOSS 0.1881\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0006 / 0006 | LOSS 0.1861\n","VALID: EPOCH 0040 / 0100 | BATCH 0001 / 0001 | LOSS 0.1820\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0001 / 0006 | LOSS 0.1846\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0002 / 0006 | LOSS 0.1845\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0003 / 0006 | LOSS 0.1806\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0004 / 0006 | LOSS 0.1839\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0005 / 0006 | LOSS 0.1881\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0006 / 0006 | LOSS 0.1881\n","VALID: EPOCH 0041 / 0100 | BATCH 0001 / 0001 | LOSS 0.1945\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0001 / 0006 | LOSS 0.1733\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0002 / 0006 | LOSS 0.1798\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0003 / 0006 | LOSS 0.1826\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0004 / 0006 | LOSS 0.1855\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0005 / 0006 | LOSS 0.1859\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0006 / 0006 | LOSS 0.1838\n","VALID: EPOCH 0042 / 0100 | BATCH 0001 / 0001 | LOSS 0.1960\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0001 / 0006 | LOSS 0.1786\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0002 / 0006 | LOSS 0.1854\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0003 / 0006 | LOSS 0.1884\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0004 / 0006 | LOSS 0.1839\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0005 / 0006 | LOSS 0.1880\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0006 / 0006 | LOSS 0.1876\n","VALID: EPOCH 0043 / 0100 | BATCH 0001 / 0001 | LOSS 0.1842\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0001 / 0006 | LOSS 0.1768\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0002 / 0006 | LOSS 0.1824\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0003 / 0006 | LOSS 0.1829\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0004 / 0006 | LOSS 0.1813\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0005 / 0006 | LOSS 0.1809\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0006 / 0006 | LOSS 0.1834\n","VALID: EPOCH 0044 / 0100 | BATCH 0001 / 0001 | LOSS 0.1888\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0001 / 0006 | LOSS 0.1944\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0002 / 0006 | LOSS 0.1868\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0003 / 0006 | LOSS 0.1840\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0004 / 0006 | LOSS 0.1820\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0005 / 0006 | LOSS 0.1826\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0006 / 0006 | LOSS 0.1811\n","VALID: EPOCH 0045 / 0100 | BATCH 0001 / 0001 | LOSS 0.1880\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0001 / 0006 | LOSS 0.1932\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0002 / 0006 | LOSS 0.1904\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0003 / 0006 | LOSS 0.1863\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0004 / 0006 | LOSS 0.1833\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0005 / 0006 | LOSS 0.1855\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0006 / 0006 | LOSS 0.1839\n","VALID: EPOCH 0046 / 0100 | BATCH 0001 / 0001 | LOSS 0.1865\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0001 / 0006 | LOSS 0.1777\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0002 / 0006 | LOSS 0.1783\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0003 / 0006 | LOSS 0.1761\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0004 / 0006 | LOSS 0.1773\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0005 / 0006 | LOSS 0.1833\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0006 / 0006 | LOSS 0.1813\n","VALID: EPOCH 0047 / 0100 | BATCH 0001 / 0001 | LOSS 0.1824\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0001 / 0006 | LOSS 0.1779\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0002 / 0006 | LOSS 0.1829\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0003 / 0006 | LOSS 0.1849\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0004 / 0006 | LOSS 0.1815\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0005 / 0006 | LOSS 0.1806\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0006 / 0006 | LOSS 0.1806\n","VALID: EPOCH 0048 / 0100 | BATCH 0001 / 0001 | LOSS 0.1935\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0001 / 0006 | LOSS 0.1698\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0002 / 0006 | LOSS 0.1769\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0003 / 0006 | LOSS 0.1797\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0004 / 0006 | LOSS 0.1782\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0005 / 0006 | LOSS 0.1800\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0006 / 0006 | LOSS 0.1800\n","VALID: EPOCH 0049 / 0100 | BATCH 0001 / 0001 | LOSS 0.1835\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0001 / 0006 | LOSS 0.1873\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0002 / 0006 | LOSS 0.1765\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0003 / 0006 | LOSS 0.1760\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0004 / 0006 | LOSS 0.1793\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0005 / 0006 | LOSS 0.1809\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0006 / 0006 | LOSS 0.1816\n","VALID: EPOCH 0050 / 0100 | BATCH 0001 / 0001 | LOSS 0.1832\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0001 / 0006 | LOSS 0.1911\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0002 / 0006 | LOSS 0.1830\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0003 / 0006 | LOSS 0.1837\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0004 / 0006 | LOSS 0.1852\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0005 / 0006 | LOSS 0.1829\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0006 / 0006 | LOSS 0.1819\n","VALID: EPOCH 0051 / 0100 | BATCH 0001 / 0001 | LOSS 0.1841\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0001 / 0006 | LOSS 0.1811\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0002 / 0006 | LOSS 0.1821\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0003 / 0006 | LOSS 0.1815\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0004 / 0006 | LOSS 0.1824\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0005 / 0006 | LOSS 0.1811\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0006 / 0006 | LOSS 0.1811\n","VALID: EPOCH 0052 / 0100 | BATCH 0001 / 0001 | LOSS 0.1855\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0001 / 0006 | LOSS 0.1756\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0002 / 0006 | LOSS 0.1816\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0003 / 0006 | LOSS 0.1782\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0004 / 0006 | LOSS 0.1781\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0005 / 0006 | LOSS 0.1804\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0006 / 0006 | LOSS 0.1794\n","VALID: EPOCH 0053 / 0100 | BATCH 0001 / 0001 | LOSS 0.1803\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0001 / 0006 | LOSS 0.1761\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0002 / 0006 | LOSS 0.1782\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0003 / 0006 | LOSS 0.1783\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0004 / 0006 | LOSS 0.1749\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0005 / 0006 | LOSS 0.1772\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0006 / 0006 | LOSS 0.1763\n","VALID: EPOCH 0054 / 0100 | BATCH 0001 / 0001 | LOSS 0.1809\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0001 / 0006 | LOSS 0.1856\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0002 / 0006 | LOSS 0.1846\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0003 / 0006 | LOSS 0.1814\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0004 / 0006 | LOSS 0.1788\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0005 / 0006 | LOSS 0.1787\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0006 / 0006 | LOSS 0.1786\n","VALID: EPOCH 0055 / 0100 | BATCH 0001 / 0001 | LOSS 0.1863\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0001 / 0006 | LOSS 0.1915\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0002 / 0006 | LOSS 0.1762\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0003 / 0006 | LOSS 0.1764\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0004 / 0006 | LOSS 0.1768\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0005 / 0006 | LOSS 0.1782\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0006 / 0006 | LOSS 0.1763\n","VALID: EPOCH 0056 / 0100 | BATCH 0001 / 0001 | LOSS 0.1804\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0001 / 0006 | LOSS 0.1614\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0002 / 0006 | LOSS 0.1800\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0003 / 0006 | LOSS 0.1759\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0004 / 0006 | LOSS 0.1748\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0005 / 0006 | LOSS 0.1760\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0006 / 0006 | LOSS 0.1737\n","VALID: EPOCH 0057 / 0100 | BATCH 0001 / 0001 | LOSS 0.1904\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0001 / 0006 | LOSS 0.2158\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0002 / 0006 | LOSS 0.1922\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0003 / 0006 | LOSS 0.1866\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0004 / 0006 | LOSS 0.1869\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0005 / 0006 | LOSS 0.1842\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0006 / 0006 | LOSS 0.1805\n","VALID: EPOCH 0058 / 0100 | BATCH 0001 / 0001 | LOSS 0.1793\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0001 / 0006 | LOSS 0.1759\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0002 / 0006 | LOSS 0.1851\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0003 / 0006 | LOSS 0.1785\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0004 / 0006 | LOSS 0.1775\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0005 / 0006 | LOSS 0.1771\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0006 / 0006 | LOSS 0.1750\n","VALID: EPOCH 0059 / 0100 | BATCH 0001 / 0001 | LOSS 0.1832\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0001 / 0006 | LOSS 0.1700\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0002 / 0006 | LOSS 0.1770\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0003 / 0006 | LOSS 0.1755\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0004 / 0006 | LOSS 0.1723\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0005 / 0006 | LOSS 0.1722\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0006 / 0006 | LOSS 0.1753\n","VALID: EPOCH 0060 / 0100 | BATCH 0001 / 0001 | LOSS 0.1915\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0001 / 0006 | LOSS 0.1971\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0002 / 0006 | LOSS 0.1954\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0003 / 0006 | LOSS 0.1863\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0004 / 0006 | LOSS 0.1819\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0005 / 0006 | LOSS 0.1792\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0006 / 0006 | LOSS 0.1788\n","VALID: EPOCH 0061 / 0100 | BATCH 0001 / 0001 | LOSS 0.1946\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0001 / 0006 | LOSS 0.1832\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0002 / 0006 | LOSS 0.1887\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0003 / 0006 | LOSS 0.1813\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0004 / 0006 | LOSS 0.1800\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0005 / 0006 | LOSS 0.1794\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0006 / 0006 | LOSS 0.1785\n","VALID: EPOCH 0062 / 0100 | BATCH 0001 / 0001 | LOSS 0.1813\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0001 / 0006 | LOSS 0.1877\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0002 / 0006 | LOSS 0.1828\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0003 / 0006 | LOSS 0.1773\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0004 / 0006 | LOSS 0.1782\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0005 / 0006 | LOSS 0.1778\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0006 / 0006 | LOSS 0.1766\n","VALID: EPOCH 0063 / 0100 | BATCH 0001 / 0001 | LOSS 0.1883\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0001 / 0006 | LOSS 0.1757\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0002 / 0006 | LOSS 0.1715\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0003 / 0006 | LOSS 0.1715\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0004 / 0006 | LOSS 0.1695\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0005 / 0006 | LOSS 0.1705\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0006 / 0006 | LOSS 0.1744\n","VALID: EPOCH 0064 / 0100 | BATCH 0001 / 0001 | LOSS 0.1854\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0001 / 0006 | LOSS 0.1593\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0002 / 0006 | LOSS 0.1689\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0003 / 0006 | LOSS 0.1768\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0004 / 0006 | LOSS 0.1740\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0005 / 0006 | LOSS 0.1724\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0006 / 0006 | LOSS 0.1729\n","VALID: EPOCH 0065 / 0100 | BATCH 0001 / 0001 | LOSS 0.1950\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0001 / 0006 | LOSS 0.1723\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0002 / 0006 | LOSS 0.1674\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0003 / 0006 | LOSS 0.1679\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0004 / 0006 | LOSS 0.1726\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0005 / 0006 | LOSS 0.1741\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0006 / 0006 | LOSS 0.1713\n","VALID: EPOCH 0066 / 0100 | BATCH 0001 / 0001 | LOSS 0.1740\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0001 / 0006 | LOSS 0.1665\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0002 / 0006 | LOSS 0.1626\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0003 / 0006 | LOSS 0.1662\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0004 / 0006 | LOSS 0.1678\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0005 / 0006 | LOSS 0.1691\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0006 / 0006 | LOSS 0.1692\n","VALID: EPOCH 0067 / 0100 | BATCH 0001 / 0001 | LOSS 0.1766\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0001 / 0006 | LOSS 0.1781\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0002 / 0006 | LOSS 0.1743\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0003 / 0006 | LOSS 0.1721\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0004 / 0006 | LOSS 0.1708\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0005 / 0006 | LOSS 0.1698\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0006 / 0006 | LOSS 0.1704\n","VALID: EPOCH 0068 / 0100 | BATCH 0001 / 0001 | LOSS 0.1758\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0001 / 0006 | LOSS 0.1712\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0002 / 0006 | LOSS 0.1639\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0003 / 0006 | LOSS 0.1679\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0004 / 0006 | LOSS 0.1676\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0005 / 0006 | LOSS 0.1670\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0006 / 0006 | LOSS 0.1684\n","VALID: EPOCH 0069 / 0100 | BATCH 0001 / 0001 | LOSS 0.1825\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0001 / 0006 | LOSS 0.1776\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0002 / 0006 | LOSS 0.1731\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0003 / 0006 | LOSS 0.1677\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0004 / 0006 | LOSS 0.1714\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0005 / 0006 | LOSS 0.1712\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0006 / 0006 | LOSS 0.1694\n","VALID: EPOCH 0070 / 0100 | BATCH 0001 / 0001 | LOSS 0.1722\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0001 / 0006 | LOSS 0.1776\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0002 / 0006 | LOSS 0.1701\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0003 / 0006 | LOSS 0.1655\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0004 / 0006 | LOSS 0.1666\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0005 / 0006 | LOSS 0.1711\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0006 / 0006 | LOSS 0.1689\n","VALID: EPOCH 0071 / 0100 | BATCH 0001 / 0001 | LOSS 0.1948\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0001 / 0006 | LOSS 0.1731\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0002 / 0006 | LOSS 0.1672\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0003 / 0006 | LOSS 0.1660\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0004 / 0006 | LOSS 0.1671\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0005 / 0006 | LOSS 0.1677\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0006 / 0006 | LOSS 0.1690\n","VALID: EPOCH 0072 / 0100 | BATCH 0001 / 0001 | LOSS 0.1803\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0001 / 0006 | LOSS 0.1577\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0002 / 0006 | LOSS 0.1709\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0003 / 0006 | LOSS 0.1705\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0004 / 0006 | LOSS 0.1680\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0005 / 0006 | LOSS 0.1693\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0006 / 0006 | LOSS 0.1685\n","VALID: EPOCH 0073 / 0100 | BATCH 0001 / 0001 | LOSS 0.1730\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0001 / 0006 | LOSS 0.1745\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0002 / 0006 | LOSS 0.1682\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0003 / 0006 | LOSS 0.1677\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0004 / 0006 | LOSS 0.1666\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0005 / 0006 | LOSS 0.1648\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0006 / 0006 | LOSS 0.1654\n","VALID: EPOCH 0074 / 0100 | BATCH 0001 / 0001 | LOSS 0.1769\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0001 / 0006 | LOSS 0.1666\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0002 / 0006 | LOSS 0.1618\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0003 / 0006 | LOSS 0.1620\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0004 / 0006 | LOSS 0.1668\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0005 / 0006 | LOSS 0.1668\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0006 / 0006 | LOSS 0.1663\n","VALID: EPOCH 0075 / 0100 | BATCH 0001 / 0001 | LOSS 0.1748\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0001 / 0006 | LOSS 0.1564\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0002 / 0006 | LOSS 0.1622\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0003 / 0006 | LOSS 0.1665\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0004 / 0006 | LOSS 0.1650\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0005 / 0006 | LOSS 0.1617\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0006 / 0006 | LOSS 0.1632\n","VALID: EPOCH 0076 / 0100 | BATCH 0001 / 0001 | LOSS 0.1803\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0001 / 0006 | LOSS 0.1842\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0002 / 0006 | LOSS 0.1683\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0003 / 0006 | LOSS 0.1634\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0004 / 0006 | LOSS 0.1644\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0005 / 0006 | LOSS 0.1648\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0006 / 0006 | LOSS 0.1637\n","VALID: EPOCH 0077 / 0100 | BATCH 0001 / 0001 | LOSS 0.1717\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0001 / 0006 | LOSS 0.1532\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0002 / 0006 | LOSS 0.1579\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0003 / 0006 | LOSS 0.1614\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0004 / 0006 | LOSS 0.1590\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0005 / 0006 | LOSS 0.1623\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0006 / 0006 | LOSS 0.1627\n","VALID: EPOCH 0078 / 0100 | BATCH 0001 / 0001 | LOSS 0.1856\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0001 / 0006 | LOSS 0.1696\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0002 / 0006 | LOSS 0.1688\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0003 / 0006 | LOSS 0.1654\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0004 / 0006 | LOSS 0.1643\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0005 / 0006 | LOSS 0.1654\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0006 / 0006 | LOSS 0.1627\n","VALID: EPOCH 0079 / 0100 | BATCH 0001 / 0001 | LOSS 0.1797\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0001 / 0006 | LOSS 0.1606\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0002 / 0006 | LOSS 0.1625\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0003 / 0006 | LOSS 0.1671\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0004 / 0006 | LOSS 0.1646\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0005 / 0006 | LOSS 0.1663\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0006 / 0006 | LOSS 0.1651\n","VALID: EPOCH 0080 / 0100 | BATCH 0001 / 0001 | LOSS 0.1930\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0001 / 0006 | LOSS 0.1562\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0002 / 0006 | LOSS 0.1561\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0003 / 0006 | LOSS 0.1597\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0004 / 0006 | LOSS 0.1616\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0005 / 0006 | LOSS 0.1628\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0006 / 0006 | LOSS 0.1638\n","VALID: EPOCH 0081 / 0100 | BATCH 0001 / 0001 | LOSS 0.1875\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0001 / 0006 | LOSS 0.1564\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0002 / 0006 | LOSS 0.1701\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0003 / 0006 | LOSS 0.1681\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0004 / 0006 | LOSS 0.1634\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0005 / 0006 | LOSS 0.1635\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0006 / 0006 | LOSS 0.1631\n","VALID: EPOCH 0082 / 0100 | BATCH 0001 / 0001 | LOSS 0.2098\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0001 / 0006 | LOSS 0.1565\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0002 / 0006 | LOSS 0.1599\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0003 / 0006 | LOSS 0.1628\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0004 / 0006 | LOSS 0.1689\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0005 / 0006 | LOSS 0.1650\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0006 / 0006 | LOSS 0.1675\n","VALID: EPOCH 0083 / 0100 | BATCH 0001 / 0001 | LOSS 0.1917\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0001 / 0006 | LOSS 0.1578\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0002 / 0006 | LOSS 0.1597\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0003 / 0006 | LOSS 0.1635\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0004 / 0006 | LOSS 0.1634\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0005 / 0006 | LOSS 0.1633\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0006 / 0006 | LOSS 0.1642\n","VALID: EPOCH 0084 / 0100 | BATCH 0001 / 0001 | LOSS 0.1913\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0001 / 0006 | LOSS 0.1580\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0002 / 0006 | LOSS 0.1619\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0003 / 0006 | LOSS 0.1698\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0004 / 0006 | LOSS 0.1653\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0005 / 0006 | LOSS 0.1632\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0006 / 0006 | LOSS 0.1621\n","VALID: EPOCH 0085 / 0100 | BATCH 0001 / 0001 | LOSS 0.1759\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0001 / 0006 | LOSS 0.1695\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0002 / 0006 | LOSS 0.1588\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0003 / 0006 | LOSS 0.1557\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0004 / 0006 | LOSS 0.1604\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0005 / 0006 | LOSS 0.1631\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0006 / 0006 | LOSS 0.1622\n","VALID: EPOCH 0086 / 0100 | BATCH 0001 / 0001 | LOSS 0.1767\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0001 / 0006 | LOSS 0.1471\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0002 / 0006 | LOSS 0.1609\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0003 / 0006 | LOSS 0.1566\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0004 / 0006 | LOSS 0.1599\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0005 / 0006 | LOSS 0.1641\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0006 / 0006 | LOSS 0.1626\n","VALID: EPOCH 0087 / 0100 | BATCH 0001 / 0001 | LOSS 0.1871\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0001 / 0006 | LOSS 0.1735\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0002 / 0006 | LOSS 0.1684\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0003 / 0006 | LOSS 0.1658\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0004 / 0006 | LOSS 0.1661\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0005 / 0006 | LOSS 0.1664\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0006 / 0006 | LOSS 0.1634\n","VALID: EPOCH 0088 / 0100 | BATCH 0001 / 0001 | LOSS 0.1836\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0001 / 0006 | LOSS 0.1526\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0002 / 0006 | LOSS 0.1493\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0003 / 0006 | LOSS 0.1591\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0004 / 0006 | LOSS 0.1598\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0005 / 0006 | LOSS 0.1619\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0006 / 0006 | LOSS 0.1626\n","VALID: EPOCH 0089 / 0100 | BATCH 0001 / 0001 | LOSS 0.1907\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0001 / 0006 | LOSS 0.1434\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0002 / 0006 | LOSS 0.1607\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0003 / 0006 | LOSS 0.1655\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0004 / 0006 | LOSS 0.1637\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0005 / 0006 | LOSS 0.1644\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0006 / 0006 | LOSS 0.1617\n","VALID: EPOCH 0090 / 0100 | BATCH 0001 / 0001 | LOSS 0.1874\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0001 / 0006 | LOSS 0.1851\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0002 / 0006 | LOSS 0.1705\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0003 / 0006 | LOSS 0.1720\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0004 / 0006 | LOSS 0.1661\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0005 / 0006 | LOSS 0.1650\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0006 / 0006 | LOSS 0.1625\n","VALID: EPOCH 0091 / 0100 | BATCH 0001 / 0001 | LOSS 0.1994\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0001 / 0006 | LOSS 0.1647\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0002 / 0006 | LOSS 0.1610\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0003 / 0006 | LOSS 0.1611\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0004 / 0006 | LOSS 0.1581\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0005 / 0006 | LOSS 0.1596\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0006 / 0006 | LOSS 0.1596\n","VALID: EPOCH 0092 / 0100 | BATCH 0001 / 0001 | LOSS 0.1782\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0001 / 0006 | LOSS 0.1526\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0002 / 0006 | LOSS 0.1570\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0003 / 0006 | LOSS 0.1582\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0004 / 0006 | LOSS 0.1611\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0005 / 0006 | LOSS 0.1602\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0006 / 0006 | LOSS 0.1587\n","VALID: EPOCH 0093 / 0100 | BATCH 0001 / 0001 | LOSS 0.1794\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0001 / 0006 | LOSS 0.1662\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0002 / 0006 | LOSS 0.1706\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0003 / 0006 | LOSS 0.1641\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0004 / 0006 | LOSS 0.1623\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0005 / 0006 | LOSS 0.1644\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0006 / 0006 | LOSS 0.1632\n","VALID: EPOCH 0094 / 0100 | BATCH 0001 / 0001 | LOSS 0.1855\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0001 / 0006 | LOSS 0.1692\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0002 / 0006 | LOSS 0.1568\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0003 / 0006 | LOSS 0.1647\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0004 / 0006 | LOSS 0.1636\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0005 / 0006 | LOSS 0.1618\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0006 / 0006 | LOSS 0.1627\n","VALID: EPOCH 0095 / 0100 | BATCH 0001 / 0001 | LOSS 0.1874\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0001 / 0006 | LOSS 0.1478\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0002 / 0006 | LOSS 0.1469\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0003 / 0006 | LOSS 0.1483\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0004 / 0006 | LOSS 0.1594\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0005 / 0006 | LOSS 0.1603\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0006 / 0006 | LOSS 0.1603\n","VALID: EPOCH 0096 / 0100 | BATCH 0001 / 0001 | LOSS 0.1899\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0001 / 0006 | LOSS 0.1673\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0002 / 0006 | LOSS 0.1618\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0003 / 0006 | LOSS 0.1605\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0004 / 0006 | LOSS 0.1629\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0005 / 0006 | LOSS 0.1607\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0006 / 0006 | LOSS 0.1573\n","VALID: EPOCH 0097 / 0100 | BATCH 0001 / 0001 | LOSS 0.2055\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0001 / 0006 | LOSS 0.1574\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0002 / 0006 | LOSS 0.1641\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0003 / 0006 | LOSS 0.1610\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0004 / 0006 | LOSS 0.1574\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0005 / 0006 | LOSS 0.1581\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0006 / 0006 | LOSS 0.1576\n","VALID: EPOCH 0098 / 0100 | BATCH 0001 / 0001 | LOSS 0.1872\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0001 / 0006 | LOSS 0.1546\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0002 / 0006 | LOSS 0.1557\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0003 / 0006 | LOSS 0.1543\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0004 / 0006 | LOSS 0.1543\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0005 / 0006 | LOSS 0.1552\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0006 / 0006 | LOSS 0.1580\n","VALID: EPOCH 0099 / 0100 | BATCH 0001 / 0001 | LOSS 0.1863\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0001 / 0006 | LOSS 0.1468\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0002 / 0006 | LOSS 0.1516\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0003 / 0006 | LOSS 0.1611\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0004 / 0006 | LOSS 0.1599\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0005 / 0006 | LOSS 0.1585\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0006 / 0006 | LOSS 0.1592\n","VALID: EPOCH 0100 / 0100 | BATCH 0001 / 0001 | LOSS 0.1997\n"]}]},{"cell_type":"code","source":["!python3 'eval.py'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sf4irDOX7Wvg","executionInfo":{"status":"ok","timestamp":1743522085377,"user_tz":-540,"elapsed":12550,"user":{"displayName":"‍홍선재(학부생-소프트웨어전공)","userId":"17464694903666453963"}},"outputId":"474a2c04-050d-41c4-b8b3-5bc1f54c8627"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-04-01 15:41:13.930788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1743522073.951884   10801 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1743522073.958506   10801 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-04-01 15:41:13.979692: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","TEST: BATCH 0001 / 0001 | LOSS 0.2089\n","AVERAGE TEST: BATCH 0001 / 0001 | LOSS 0.2089\n"]}]},{"cell_type":"code","source":["!python3 'display_results.py'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HCe4e-RmA4gq","executionInfo":{"status":"ok","timestamp":1743522093346,"user_tz":-540,"elapsed":1409,"user":{"displayName":"‍홍선재(학부생-소프트웨어전공)","userId":"17464694903666453963"}},"outputId":"cf54495d-5f2b-4afe-8094-d9d9c2b359e1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Figure(640x480)\n"]}]},{"cell_type":"code","source":["!python3 \"train.py\" \\\n","--lr 1e-2 --batch_size 2 --num_epoch 300 \\\n","--data_dir \"datasets\" \\\n","--ckpt_dir \"checkpoint_v2\" \\\n","--log_dir \"log_v2\" \\\n","--result_dir \"result_v2\" \\\n","--mode \"test\" \\\n","--train_continue \"off\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpmygtSBA9Lg","executionInfo":{"status":"ok","timestamp":1743522117492,"user_tz":-540,"elapsed":11642,"user":{"displayName":"‍홍선재(학부생-소프트웨어전공)","userId":"17464694903666453963"}},"outputId":"2ba46946-1cfd-479f-c125-697e05fb6e08"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-04-01 15:41:46.266074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1743522106.287091   10971 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1743522106.293549   10971 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-04-01 15:41:46.314571: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","learning rate: 1.0000e-02\n","batch size: 2\n","number of epoch: 300\n","data dir: datasets\n","ckpt dir: checkpoint_v2\n","log dir: log_v2\n","result dir: result_v2\n","mode: test\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","TEST: BATCH 0001 / 0002 | LOSS 0.7070\n","TEST: BATCH 0002 / 0002 | LOSS 0.7078\n","AVERAGE TEST: BATCH 0002 / 0002 | LOSS 0.7078\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"D3W1zJXOBAkw"},"execution_count":null,"outputs":[]}]}