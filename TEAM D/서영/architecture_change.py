# -*- coding: utf-8 -*-
"""architecture_change

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dfuQ5mhJu-_Sqc8bgkvfzIGZhTiMsCbN

# 1. 레이어 구성 변경
"""

import torch.nn as nn

# 간단한 VGG 구조: conv_2_block만 사용하고 블록 수를 줄임
class VGG_Simple(nn.Module):
    def __init__(self, base_dim=64, num_classes=10):
        super(VGG_Simple, self).__init__()
        self.feature = nn.Sequential(
            conv_2_block(3, base_dim),  # 입력 3채널 -> base_dim
            conv_2_block(base_dim, base_dim * 2),  # 채널 수 2배 증가
            conv_2_block(base_dim * 2, base_dim * 4)  # 총 3블록만 사용
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim * 4 * 4 * 4, 512),  # CIFAR-10은 32x32 → 4x4로 줄어듦
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.fc_layer(x)
        return x

"""- 기존 VGG보다 **얕은 구조**
- `conv_2_block`만 사용하며 블록 수를 3개로 제한
- Fully Connected Layer도 간단히 구성
- CIFAR-10과 같이 입력 크기가 작은 데이터셋에 적합

#  2. BatchNorm 추가된 VGG
"""

# BatchNorm이 들어간 conv_2_block 정의
def conv_2_block_bn(in_dim, out_dim):
    return nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),
        nn.BatchNorm2d(out_dim),  # 배치 정규화 추가
        nn.ReLU(),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.BatchNorm2d(out_dim),  # 배치 정규화 추가
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

# BatchNorm이 포함된 VGG 구조
class VGG_BatchNorm(nn.Module):
    def __init__(self, base_dim=64, num_classes=10):
        super(VGG_BatchNorm, self).__init__()
        self.feature = nn.Sequential(
            conv_2_block_bn(3, base_dim),
            conv_2_block_bn(base_dim, base_dim * 2),
            conv_2_block_bn(base_dim * 2, base_dim * 4)
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim * 4 * 4 * 4, 512),
            nn.ReLU(),
            nn.Dropout(),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layer(x)
        return x

"""- 기존 VGG 구조를 간소화하면서 **Batch Normalization**을 적용한 CNN 모델
- 배치 정규화를 통해 학습 안정성 향상 및 수렴 속도 증가
- CIFAR-10과 같은 소형 이미지 데이터셋에 적합한 구조

# 3. Dropout 추가 (Convolution 사이에)
"""

# Conv 레이어 사이에 Dropout 삽입
def conv_2_block_dropout(in_dim, out_dim):
    return nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Dropout(0.2),  # 20% 확률로 뉴런 무시
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

# Dropout이 포함된 VGG 구조
class VGG_Dropout(nn.Module):
    def __init__(self, base_dim=64, num_classes=10):
        super(VGG_Dropout, self).__init__()
        self.feature = nn.Sequential(
            conv_2_block_dropout(3, base_dim),
            conv_2_block_dropout(base_dim, base_dim * 2),
            conv_2_block_dropout(base_dim * 2, base_dim * 4)
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim * 4 * 4 * 4, 512),
            nn.ReLU(),
            nn.Dropout(0.5),  # FC에서도 Dropout 적용
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layer(x)
        return x

"""- 기존 VGG 구조를 간소화하면서 **Dropout을 Convolution 사이에도 삽입**
- 특징 추출 과정에서의 과적합 방지를 강화한 구조
- CIFAR-10처럼 이미지가 작고 데이터 수가 적을 때 과적합 방지에 효과적

# 4. ReLU → LeakyReLU 변경
"""

# LeakyReLU 사용한 Conv 블록
def conv_2_block_leaky(in_dim, out_dim):
    return nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),
        nn.LeakyReLU(0.1),  # ReLU 대신 LeakyReLU
        nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1),
        nn.LeakyReLU(0.1),
        nn.MaxPool2d(kernel_size=2, stride=2)
    )

# LeakyReLU가 포함된 VGG 구조
class VGG_Leaky(nn.Module):
    def __init__(self, base_dim=64, num_classes=10):
        super(VGG_Leaky, self).__init__()
        self.feature = nn.Sequential(
            conv_2_block_leaky(3, base_dim),
            conv_2_block_leaky(base_dim, base_dim * 2),
            conv_2_block_leaky(base_dim * 2, base_dim * 4)
        )
        self.fc_layer = nn.Sequential(
            nn.Linear(base_dim * 4 * 4 * 4, 512),
            nn.LeakyReLU(0.1),
            nn.Dropout(),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.feature(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layer(x)
        return x

"""- 일반적인 ReLU 대신 **LeakyReLU**를 사용한 VGG 구조
- ReLU의 문제점인 "죽은 뉴런"을 방지하며 **더 안정적인 학습 가능**
- CIFAR-10처럼 작은 이미지 데이터셋에 적용하기 좋은 가벼운 구조

# 5. Global Average Pooling 사용
"""

# GAP (Global Average Pooling) 적용 버전
class VGG_GAP(nn.Module):
    def __init__(self, base_dim=64, num_classes=10):
        super(VGG_GAP, self).__init__()
        self.feature = nn.Sequential(
            conv_2_block(3, base_dim),
            conv_2_block(base_dim, base_dim * 2),
            conv_2_block(base_dim * 2, base_dim * 4)
        )
        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Feature map → 1x1으로 줄임
        self.classifier = nn.Linear(base_dim * 4, num_classes)  # FC 대신 간단한 분류

    def forward(self, x):
        x = self.feature(x)
        x = self.pool(x)  # GAP
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

"""- 기존 VGG 구조에서 Fully Connected Layer 대신 **Global Average Pooling (GAP)** 사용
- FC 파라미터 수를 크게 줄여서 모델 경량화 및 과적합 감소에 도움
- CIFAR-10과 같은 소형 이미지 분류에 적합
"""

