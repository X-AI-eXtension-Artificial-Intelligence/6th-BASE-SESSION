import os
import wandb
import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from model import UNet
from dataset import DatasetForSeg, data_transform
from metrics import calculate_IOU, calculate_errors
from hParams import get_hParams
'''
(*) with torch.cuda.amp
- PyTorchì—ì„œ ìë™ í˜¼í•© ì •ë°€ë„ë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë“ˆ
- í˜¼í•© ì •ë°€ë„ë¥¼ ì‚¬ìš©í•˜ë©´ GPUì˜ ê³„ì‚° ìì›ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŒ
- í˜¼í•© ì •ë°€ë„ : ë¶€ë™ ì†Œìˆ˜ì  ì—°ì‚°ì—ì„œ 16ë¹„íŠ¸(half precision)ì™€ 32ë¹„íŠ¸(single precision)ë¥¼ ì ì ˆíˆ í˜¼í•©í•´ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì˜ë¯¸

(*) autocast():
- íŠ¹ì • ë²”ìœ„ ë‚´ì—ì„œ í˜¼í•© ì •ë°€ë„ë¥¼ ìë™ìœ¼ë¡œ ì ìš©
- ì´ ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œëŠ” ëª¨ë¸ì˜ ì—°ì‚°ì´ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•´ ìë™ìœ¼ë¡œ FP16(half precision)ê³¼ FP32(single precision)ë¡œ ì „í™˜
- FP16 ì—°ì‚°ì€ FP32ë³´ë‹¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ê³ , ì—°ì‚° ì†ë„ê°€ ë” ë¹ ë¦„
- ì •ë°€ë„ ìœ ì§€: ëª¨ë“  ì—°ì‚°ì„ FP16ìœ¼ë¡œ ìˆ˜í–‰í•  ê²½ìš°, ì •ë°€ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆì§€ë§Œ, autocast()ë¡œ í•„ìš”í•œ ì—°ì‚°ì„ FP32ë¡œ ìœ ì§€í•¨ìœ¼ë¡œì¨ ì •ë°€ë„ ë³´ì¥
- forward ë° loss ê³„ì‚°ì´ ìë™ìœ¼ë¡œ í˜¼í•© ì •ë°€ë„ë¡œ ìˆ˜í–‰ë˜ë¡œë¡ forward ê³„ì‚°ì— ì´ë¥¼ ì‚¬ìš©í•¨

ì—°ì‚°ì—ì„œì˜ ì •ë°€ë„ (Precision in Computation)
- ë¶€ë™ì†Œìˆ˜ì (Floating Point) ìˆ«ìë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ í‘œí˜„í•˜ê³  ê³„ì‚°í•˜ëŠ”ê°€?
-> ë°ì´í„° í‘œí˜„ ë°©ì‹(FP16, FP32, FP64 ë“±)ê³¼ ì—°ì‚°ì˜ ì •í™•ì„±ì— ê´€ë ¨ë¨.

ğŸ“Œ 
FP16(16ë¹„íŠ¸): ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ê³  ì—°ì‚° ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ, í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê°’ì˜ ë²”ìœ„ê°€ ì¢ê³  ë°˜ì˜¬ë¦¼ ì˜¤ë¥˜ê°€ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
FP32(32ë¹„íŠ¸): ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë¨
FP64(64ë¹„íŠ¸): ë§¤ìš° ë†’ì€ ì •ë°€ë„ë¥¼ ì œê³µí•˜ì§€ë§Œ ì—°ì‚° ì†ë„ê°€ ëŠë¦¬ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¼

ğŸ“Œ ì—°ì‚° ì •ë°€ë„ê°€ ì¤‘ìš”í•œ ì´ìœ 
FP16ì„ ì‚¬ìš©í•  ê²½ìš° ì‘ì€ ìˆ«ìê°€ 0ìœ¼ë¡œ ë³€í•˜ëŠ” ì–¸ë”í”Œë¡œìš°(Underflow) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
í° ìˆ«ìê°€ ë„ˆë¬´ ì»¤ì ¸ì„œ ê³„ì‚°í•  ìˆ˜ ì—†ëŠ” ì˜¤ë²„í”Œë¡œìš°(Overflow) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
í•™ìŠµ ê³¼ì •ì—ì„œ ì˜¤ì°¨ê°€ ëˆ„ì ë  ê°€ëŠ¥ì„±ì´ ìˆìŒ
'''
def train_model(batch_size, learning_rate, num_epoch, model_name):
    device = torch.device("cuda" if torch.cuda.is_available() else 'cpu')
    print(num_epoch)
    print('## device: ', device)
    # data
    data_dir = "./data/"
    train_dir = os.path.join(data_dir, 'train')
    test_dir = os.path.join(data_dir, 'test')

    transform = data_transform()
    train_set = DatasetForSeg(data_dir=train_dir, transform=transform)
    test_set = DatasetForSeg(data_dir=test_dir, transform=transform)

    # DataLoader : ë¯¸ë‹ˆë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì œê³µ
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)

    # Trainer
    model = UNet(in_channel=1, out_channel=1).to(device) # grayscale
    loss_func = nn.BCEWithLogitsLoss().to(device)

    # UNetì—ì„œëŠ” SGD, momentum=0.99ì˜€ì§€ë§Œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ AdamW ì‚¬ìš©
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scaler = torch.cuda.amp.GradScaler()
    ## wandb
    wandb.init(project="unet-training", name=f"epoch{num_epoch}-batch{batch_size}", config={
        "epochs": num_epoch,
        "batch_size": batch_size,
        "optimizer": optimizer,
        "learning_rate": learning_rate,
    })

    print('## start training ! ##')
    loss_arr = []
    for i in tqdm(range(num_epoch), total=num_epoch, desc='training...'):
        for batch, data in enumerate(train_loader):
            model.train()
            inputs = data['input'].to(device, non_blocking=True)
            input_canny = data['input_canny'].to(device, non_blocking=True)
            label = data['label'].to(device, non_blocking=True) # ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ ë° CUDA ìŠ¤íŠ¸ë¦¬ë° í™œìš©
            label = (label + 1) / 2
            optimizer.zero_grad()

            with torch.cuda.amp.autocast():
                output = model(inputs, input_canny)  # forward
                loss = loss_func(output, label)

            pred_mask = (output > 0.5).float()
            iou = calculate_IOU(label, pred_mask)
            pixel_error, rand_error, warping_error = calculate_errors(label, pred_mask)
            wandb.log({"IOU": iou, "epoch": i})
            wandb.log({"Pixel Error": pixel_error, "epoch": i})
            wandb.log({"Rand Error": rand_error, "epoch": i})
            wandb.log({"Warping Error": warping_error, "epoch": i})
   
            # backward
            # AMP ìŠ¤ì¼€ì¼ë§ ì ìš©
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            wandb.log({"train_loss": loss.item(), "epoch": i})

            # ë°°ì¹˜ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬
            del inputs, label, output, pred_mask, loss
            torch.cuda.empty_cache()

        # 10 ì—í¬í¬ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
        if i % 10 == 0:
            print(f'Epoch {i} completed.')

            model.eval()
            with torch.no_grad():
                # --- Segmentation ì´ë¯¸ì§€ ë¡œê¹… ---
                # ë‹¨ì¼ ë°ì´í„°ì´ë¯€ë¡œ ì•ì— ë°°ì¹˜ ì°¨ì› ì¶”ê°€ unsqueeze(0)
                inputs_val = test_set[0]['input'].unsqueeze(0).to(device)
                label_val = test_set[0]['label'].unsqueeze(0).to(device)
                input_canny_val = test_set[0]['input_canny'].unsqueeze(0).to(device)
                label_val = (label_val + 1) / 2
                output_val = model(inputs_val, input_canny_val)

                pred_mask = output_val.squeeze(1)  # (batch, H, W) -> ì˜ˆì¸¡ëœ segmentation mask
                label_mask = label_val.squeeze(1)  # GT mask (batch, H, W)
                # canny_mask = input_canny_val.squeeze(1)

                pred_mask_np = pred_mask[0].cpu().detach().numpy()  # .cpu().detach()ë¥¼ ì¶”ê°€í•˜ì—¬ GPUì—ì„œ CPUë¡œ ì´ë™ í›„ numpyë¡œ ë³€í™˜
                label_mask_np = label_mask[0].cpu().detach().numpy()
                # canny_mask_np = canny_mask[0].cpu().detach().numpy()
                
                wandb.log({
                    "Predicted Mask": wandb.Image(pred_mask_np, caption="Prediction"),
                    "Ground Truth": wandb.Image(label_mask_np, caption="Ground Truth"),
                })
                # í‰ê°€ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                del inputs_val, label_val, output_val, input_canny_val, pred_mask, label_mask
                torch.cuda.empty_cache()

    wandb.finish()

    os.makedirs('model/', exist_ok=True)
    torch.save(model.state_dict(), f'model/{model_name}_epoch{num_epoch}.pth') # save_model_name default : unet_vanilla

if __name__ == '__main__':
    args = get_hParams()
    train_model(
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        num_epoch=args.num_epoch,
        model_name=args.model_name)
