import os
import wandb
import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm
from torch.utils.data import DataLoader

from model import UNet
from dataset import DatasetForSeg, data_transform
from metrics import calculate_IOU, calculate_dice_coefficient
from dice_loss import DiceLoss
'''
(*) with torch.cuda.amp
- PyTorchì—ì„œ ìë™ í˜¼í•© ì •ë°€ë„ë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë“ˆ
- í˜¼í•© ì •ë°€ë„ë¥¼ ì‚¬ìš©í•˜ë©´ GPUì˜ ê³„ì‚° ìì›ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŒ
- í˜¼í•© ì •ë°€ë„ : ë¶€ë™ ì†Œìˆ˜ì  ì—°ì‚°ì—ì„œ 16ë¹„íŠ¸(half precision)ì™€ 32ë¹„íŠ¸(single precision)ë¥¼ ì ì ˆíˆ í˜¼í•©í•´ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì˜ë¯¸

(*) autocast():
- íŠ¹ì • ë²”ìœ„ ë‚´ì—ì„œ í˜¼í•© ì •ë°€ë„ë¥¼ ìë™ìœ¼ë¡œ ì ìš©
- ì´ ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œëŠ” ëª¨ë¸ì˜ ì—°ì‚°ì´ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•´ ìë™ìœ¼ë¡œ FP16(half precision)ê³¼ FP32(single precision)ë¡œ ì „í™˜
- FP16 ì—°ì‚°ì€ FP32ë³´ë‹¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ê³ , ì—°ì‚° ì†ë„ê°€ ë” ë¹ ë¦„
- ì •ë°€ë„ ìœ ì§€: ëª¨ë“  ì—°ì‚°ì„ FP16ìœ¼ë¡œ ìˆ˜í–‰í•  ê²½ìš°, ì •ë°€ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆì§€ë§Œ, autocast()ë¡œ í•„ìš”í•œ ì—°ì‚°ì„ FP32ë¡œ ìœ ì§€í•¨ìœ¼ë¡œì¨ ì •ë°€ë„ ë³´ì¥
- forward ë° loss ê³„ì‚°ì´ ìë™ìœ¼ë¡œ í˜¼í•© ì •ë°€ë„ë¡œ ìˆ˜í–‰ë˜ë¡œë¡ forward ê³„ì‚°ì— ì´ë¥¼ ì‚¬ìš©í•¨

ì—°ì‚°ì—ì„œì˜ ì •ë°€ë„ (Precision in Computation)
ğŸ’¡ ì •ë°€ë„ë€?
ğŸ‘‰ ë¶€ë™ì†Œìˆ˜ì (Floating Point) ìˆ«ìë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ í‘œí˜„í•˜ê³  ê³„ì‚°í•˜ëŠ”ê°€?
ğŸ‘‰ ë°ì´í„° í‘œí˜„ ë°©ì‹(FP16, FP32, FP64 ë“±)ê³¼ ì—°ì‚°ì˜ ì •í™•ì„±ì— ê´€ë ¨ë¨.

ğŸ“Œ 
FP16(16ë¹„íŠ¸): ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ê³  ì—°ì‚° ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ, í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê°’ì˜ ë²”ìœ„ê°€ ì¢ê³  ë°˜ì˜¬ë¦¼ ì˜¤ë¥˜ê°€ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ.
FP32(32ë¹„íŠ¸): ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë¨.
FP64(64ë¹„íŠ¸): ë§¤ìš° ë†’ì€ ì •ë°€ë„ë¥¼ ì œê³µí•˜ì§€ë§Œ ì—°ì‚° ì†ë„ê°€ ëŠë¦¬ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¼.

ğŸ“Œ ì—°ì‚° ì •ë°€ë„ê°€ ì¤‘ìš”í•œ ì´ìœ 
FP16ì„ ì‚¬ìš©í•  ê²½ìš° ì‘ì€ ìˆ«ìê°€ 0ìœ¼ë¡œ ë³€í•˜ëŠ” ì–¸ë”í”Œë¡œìš°(Underflow) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.
í° ìˆ«ìê°€ ë„ˆë¬´ ì»¤ì ¸ì„œ ê³„ì‚°í•  ìˆ˜ ì—†ëŠ” ì˜¤ë²„í”Œë¡œìš°(Overflow) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.
í•™ìŠµ ê³¼ì •ì—ì„œ ì˜¤ì°¨ê°€ ëˆ„ì ë  ê°€ëŠ¥ì„±ì´ ìˆìŒ.
'''
def train_model(setting_config: dict):
    # setting
    batch_size = setting_config['batch_size']
    learning_rate = setting_config['learning_rate']
    num_epoch = setting_config['num_epoch']
    device = setting_config['device']
    print('using device: ', device)
    # data
    data_dir = "./data/"
    train_dir = os.path.join(data_dir, 'train')
    test_dir = os.path.join(data_dir, 'test')

    transform = data_transform()
    train_set = DatasetForSeg(data_dir=train_dir, transform=transform)
    test_set = DatasetForSeg(data_dir=test_dir, transform=transform)

    # DataLoader : ë¯¸ë‹ˆë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì œê³µ
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)

    # Trainer
    model = UNet(in_channel=1, out_channel=1).to(device) # grayscale
    # loss_func = DiceLoss().to(device)
    loss_func = nn.BCEWithLogitsLoss().to(device)

    # UNetì—ì„œëŠ” SGD, momentum=0.99ì˜€ì§€ë§Œ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ AdamW ì‚¬ìš©
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99)
    scaler = torch.cuda.amp.GradScaler()
    ## wandb
    wandb.init(project="unet-training", name=f"BCELoss-epoch{num_epoch}-batch{batch_size}", config={
        "epochs": num_epoch,
        "batch_size": batch_size,
        "optimizer": optimizer,
        "learning_rate": learning_rate,
    })

    print('## start training ! ##')
    loss_arr = []
    for i in tqdm(range(num_epoch), total=num_epoch, desc='training...'):
        for batch, data in enumerate(train_loader):
            model.train()
            inputs = data['input'].to(device, non_blocking=True)
            label = data['label'].to(device, non_blocking=True) # ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ ë° CUDA ìŠ¤íŠ¸ë¦¬ë° í™œìš©
            label = (label + 1) / 2
            optimizer.zero_grad()

            with torch.cuda.amp.autocast():
                output = model(inputs)  # forward
                loss = loss_func(output, label)

            pred_mask = (output > 0.5).float()
            iou = calculate_IOU(label, pred_mask)
            dice_coefficient = calculate_dice_coefficient(label, pred_mask)
            wandb.log({"IOU": iou, "epoch": i})
            wandb.log({"DSC": dice_coefficient, "epoch": i})

            # backward
            # AMP ìŠ¤ì¼€ì¼ë§ ì ìš©
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            wandb.log({"train_loss": loss.item(), "epoch": i})

            # ë°°ì¹˜ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬
            del inputs, label, output, pred_mask, loss
            torch.cuda.empty_cache()
            # loss.backward()
            # optimizer.step()

        # 10 ì—í¬í¬ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
        if i % 10 == 0:
            print(f'Epoch {i} completed.')

            model.eval()
            with torch.no_grad():
                # --- Segmentation ì´ë¯¸ì§€ ë¡œê¹… ---
                # ë‹¨ì¼ ë°ì´í„°ì´ë¯€ë¡œ ì•ì— ë°°ì¹˜ ì°¨ì› ì¶”ê°€ unsqueeze(0)
                inputs_val = test_set[0]['input'].unsqueeze(0).to(device)
                label_val = test_set[0]['label'].unsqueeze(0).to(device)
                label_val = (label_val + 1) / 2
                output_val = model(inputs_val)

                pred_mask = output_val.squeeze(1)  # (batch, H, W) -> ì˜ˆì¸¡ëœ segmentation mask
                label_mask = label_val.squeeze(1)  # GT mask (batch, H, W)

                pred_mask_np = pred_mask[0].cpu().detach().numpy()  # .cpu().detach()ë¥¼ ì¶”ê°€í•˜ì—¬ GPUì—ì„œ CPUë¡œ ì´ë™ í›„ numpyë¡œ ë³€í™˜
                label_mask_np = label_mask[0].cpu().detach().numpy()

                wandb.log({
                    "Predicted Mask": wandb.Image(pred_mask_np, caption="Prediction"),
                    "Ground Truth": wandb.Image(label_mask_np, caption="Ground Truth"),
                })
                # í‰ê°€ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                del inputs_val, label_val, output_val, pred_mask, label_mask
                torch.cuda.empty_cache()

    wandb.finish()


    # í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ ì €ì¥
    os.makedirs('model/', exist_ok=True)
    torch.save(model.state_dict(), setting_config['save_model_path'])

if __name__ == '__main__':
    setting_config = {
        "batch_size": 8,
        "learning_rate": 0.0001,
        "num_epoch": 300,
        "device": torch.device("cuda" if torch.cuda.is_available() else 'cpu'),
        "save_model_path": "./model/unet_BCELoss.pth"
    }
    train_model(setting_config)
