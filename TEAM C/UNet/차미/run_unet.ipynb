{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"Y8ter2UpdnFC"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{},"colab_type":"code","id":"qkaSfws4RIHf"},"outputs":[{"data":{"application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6007, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["tensorboard --logdir='/home/work/deep/xai/3주차과제/log'"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{},"colab_type":"code","id":"4ZtjGhZhdmBE"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-03-25 06:11:01.905258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-25 06:11:01.905328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-25 06:11:01.906177: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-25 06:11:01.911740: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-03-25 06:11:02.815199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","learning rate: 1.0000e-03\n","batch size: 4\n","number of epoch: 100\n","data dir: ./datasets\n","ckpt dir: ./checkpoint\n","log dir: ./log\n","result dir: ./result\n","mode: train\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0001 / 0006 | LOSS 0.7259\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0002 / 0006 | LOSS 0.6623\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0003 / 0006 | LOSS 0.6266\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0004 / 0006 | LOSS 0.6048\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0005 / 0006 | LOSS 0.5883\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0006 / 0006 | LOSS 0.5666\n","VALID: EPOCH 0001 / 0100 | BATCH 0001 / 0001 | LOSS 0.6413\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0001 / 0006 | LOSS 0.4947\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0002 / 0006 | LOSS 0.4814\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0003 / 0006 | LOSS 0.4711\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0004 / 0006 | LOSS 0.4557\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0005 / 0006 | LOSS 0.4450\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0006 / 0006 | LOSS 0.4351\n","VALID: EPOCH 0002 / 0100 | BATCH 0001 / 0001 | LOSS 0.5059\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0001 / 0006 | LOSS 0.3836\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0002 / 0006 | LOSS 0.3820\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0003 / 0006 | LOSS 0.3763\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0004 / 0006 | LOSS 0.3744\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0005 / 0006 | LOSS 0.3690\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0006 / 0006 | LOSS 0.3652\n","VALID: EPOCH 0003 / 0100 | BATCH 0001 / 0001 | LOSS 0.3852\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0001 / 0006 | LOSS 0.3533\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0002 / 0006 | LOSS 0.3457\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0003 / 0006 | LOSS 0.3444\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0004 / 0006 | LOSS 0.3386\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0005 / 0006 | LOSS 0.3355\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0006 / 0006 | LOSS 0.3342\n","VALID: EPOCH 0004 / 0100 | BATCH 0001 / 0001 | LOSS 0.4293\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0001 / 0006 | LOSS 0.3164\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0002 / 0006 | LOSS 0.3171\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0003 / 0006 | LOSS 0.3133\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0004 / 0006 | LOSS 0.3100\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0005 / 0006 | LOSS 0.3088\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0006 / 0006 | LOSS 0.3074\n","VALID: EPOCH 0005 / 0100 | BATCH 0001 / 0001 | LOSS 0.3736\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0001 / 0006 | LOSS 0.3036\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0002 / 0006 | LOSS 0.3022\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0003 / 0006 | LOSS 0.2975\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0004 / 0006 | LOSS 0.2929\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0005 / 0006 | LOSS 0.2898\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0006 / 0006 | LOSS 0.2884\n","VALID: EPOCH 0006 / 0100 | BATCH 0001 / 0001 | LOSS 0.3284\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0001 / 0006 | LOSS 0.2903\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0002 / 0006 | LOSS 0.2797\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0003 / 0006 | LOSS 0.2853\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0004 / 0006 | LOSS 0.2814\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0005 / 0006 | LOSS 0.2802\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0006 / 0006 | LOSS 0.2774\n","VALID: EPOCH 0007 / 0100 | BATCH 0001 / 0001 | LOSS 0.2848\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0001 / 0006 | LOSS 0.2611\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0002 / 0006 | LOSS 0.2667\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0003 / 0006 | LOSS 0.2701\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0004 / 0006 | LOSS 0.2662\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0005 / 0006 | LOSS 0.2683\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0006 / 0006 | LOSS 0.2674\n","VALID: EPOCH 0008 / 0100 | BATCH 0001 / 0001 | LOSS 0.2918\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0001 / 0006 | LOSS 0.2509\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0002 / 0006 | LOSS 0.2579\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0003 / 0006 | LOSS 0.2528\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0004 / 0006 | LOSS 0.2532\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0005 / 0006 | LOSS 0.2523\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0006 / 0006 | LOSS 0.2510\n","VALID: EPOCH 0009 / 0100 | BATCH 0001 / 0001 | LOSS 0.2929\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0001 / 0006 | LOSS 0.2409\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0002 / 0006 | LOSS 0.2383\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0003 / 0006 | LOSS 0.2427\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0004 / 0006 | LOSS 0.2400\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0005 / 0006 | LOSS 0.2396\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0006 / 0006 | LOSS 0.2400\n","VALID: EPOCH 0010 / 0100 | BATCH 0001 / 0001 | LOSS 0.2868\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0001 / 0006 | LOSS 0.2481\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0002 / 0006 | LOSS 0.2438\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0003 / 0006 | LOSS 0.2374\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0004 / 0006 | LOSS 0.2393\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0005 / 0006 | LOSS 0.2385\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0006 / 0006 | LOSS 0.2368\n","VALID: EPOCH 0011 / 0100 | BATCH 0001 / 0001 | LOSS 0.2576\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0001 / 0006 | LOSS 0.2191\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0002 / 0006 | LOSS 0.2263\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0003 / 0006 | LOSS 0.2260\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0004 / 0006 | LOSS 0.2299\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0005 / 0006 | LOSS 0.2304\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0006 / 0006 | LOSS 0.2301\n","VALID: EPOCH 0012 / 0100 | BATCH 0001 / 0001 | LOSS 0.2478\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0001 / 0006 | LOSS 0.2313\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0002 / 0006 | LOSS 0.2221\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0003 / 0006 | LOSS 0.2225\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0004 / 0006 | LOSS 0.2190\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0005 / 0006 | LOSS 0.2230\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0006 / 0006 | LOSS 0.2227\n","VALID: EPOCH 0013 / 0100 | BATCH 0001 / 0001 | LOSS 0.2412\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0001 / 0006 | LOSS 0.2129\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0002 / 0006 | LOSS 0.2189\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0003 / 0006 | LOSS 0.2185\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0004 / 0006 | LOSS 0.2179\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0005 / 0006 | LOSS 0.2179\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0006 / 0006 | LOSS 0.2180\n","VALID: EPOCH 0014 / 0100 | BATCH 0001 / 0001 | LOSS 0.2361\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0001 / 0006 | LOSS 0.2082\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0002 / 0006 | LOSS 0.2082\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0003 / 0006 | LOSS 0.2220\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0004 / 0006 | LOSS 0.2197\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0005 / 0006 | LOSS 0.2172\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0006 / 0006 | LOSS 0.2165\n","VALID: EPOCH 0015 / 0100 | BATCH 0001 / 0001 | LOSS 0.2354\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0001 / 0006 | LOSS 0.2140\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0002 / 0006 | LOSS 0.2089\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0003 / 0006 | LOSS 0.2092\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0004 / 0006 | LOSS 0.2138\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0005 / 0006 | LOSS 0.2112\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0006 / 0006 | LOSS 0.2120\n","VALID: EPOCH 0016 / 0100 | BATCH 0001 / 0001 | LOSS 0.2249\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0001 / 0006 | LOSS 0.2137\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0002 / 0006 | LOSS 0.2054\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0003 / 0006 | LOSS 0.2092\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0004 / 0006 | LOSS 0.2088\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0005 / 0006 | LOSS 0.2095\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0006 / 0006 | LOSS 0.2085\n","VALID: EPOCH 0017 / 0100 | BATCH 0001 / 0001 | LOSS 0.2319\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0001 / 0006 | LOSS 0.2113\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0002 / 0006 | LOSS 0.2049\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0003 / 0006 | LOSS 0.2036\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0004 / 0006 | LOSS 0.2026\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0005 / 0006 | LOSS 0.2044\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0006 / 0006 | LOSS 0.2065\n","VALID: EPOCH 0018 / 0100 | BATCH 0001 / 0001 | LOSS 0.2279\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0001 / 0006 | LOSS 0.2075\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0002 / 0006 | LOSS 0.2111\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0003 / 0006 | LOSS 0.2058\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0004 / 0006 | LOSS 0.2064\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0005 / 0006 | LOSS 0.2052\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0006 / 0006 | LOSS 0.2035\n","VALID: EPOCH 0019 / 0100 | BATCH 0001 / 0001 | LOSS 0.2128\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0001 / 0006 | LOSS 0.2044\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0002 / 0006 | LOSS 0.2059\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0003 / 0006 | LOSS 0.2013\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0004 / 0006 | LOSS 0.1984\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0005 / 0006 | LOSS 0.2037\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0006 / 0006 | LOSS 0.2026\n","VALID: EPOCH 0020 / 0100 | BATCH 0001 / 0001 | LOSS 0.2250\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0001 / 0006 | LOSS 0.1965\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0002 / 0006 | LOSS 0.1925\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0003 / 0006 | LOSS 0.1937\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0004 / 0006 | LOSS 0.2008\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0005 / 0006 | LOSS 0.2011\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0006 / 0006 | LOSS 0.1987\n","VALID: EPOCH 0021 / 0100 | BATCH 0001 / 0001 | LOSS 0.2352\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0001 / 0006 | LOSS 0.1925\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0002 / 0006 | LOSS 0.2003\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0003 / 0006 | LOSS 0.1987\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0004 / 0006 | LOSS 0.1970\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0005 / 0006 | LOSS 0.2048\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0006 / 0006 | LOSS 0.2043\n","VALID: EPOCH 0022 / 0100 | BATCH 0001 / 0001 | LOSS 0.2183\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0001 / 0006 | LOSS 0.1764\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0002 / 0006 | LOSS 0.1860\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0003 / 0006 | LOSS 0.1890\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0004 / 0006 | LOSS 0.1950\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0005 / 0006 | LOSS 0.1943\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0006 / 0006 | LOSS 0.1969\n","VALID: EPOCH 0023 / 0100 | BATCH 0001 / 0001 | LOSS 0.2217\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0001 / 0006 | LOSS 0.1872\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0002 / 0006 | LOSS 0.2008\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0003 / 0006 | LOSS 0.2027\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0004 / 0006 | LOSS 0.1975\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0005 / 0006 | LOSS 0.1947\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0006 / 0006 | LOSS 0.1949\n","VALID: EPOCH 0024 / 0100 | BATCH 0001 / 0001 | LOSS 0.2231\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0001 / 0006 | LOSS 0.2039\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0002 / 0006 | LOSS 0.1943\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0003 / 0006 | LOSS 0.2010\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0004 / 0006 | LOSS 0.1986\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0005 / 0006 | LOSS 0.1984\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0006 / 0006 | LOSS 0.1968\n","VALID: EPOCH 0025 / 0100 | BATCH 0001 / 0001 | LOSS 0.2322\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0001 / 0006 | LOSS 0.1979\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0002 / 0006 | LOSS 0.1885\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0003 / 0006 | LOSS 0.1871\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0004 / 0006 | LOSS 0.1943\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0005 / 0006 | LOSS 0.1933\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0006 / 0006 | LOSS 0.1957\n","VALID: EPOCH 0026 / 0100 | BATCH 0001 / 0001 | LOSS 0.2286\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0001 / 0006 | LOSS 0.1813\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0002 / 0006 | LOSS 0.1954\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0003 / 0006 | LOSS 0.1930\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0004 / 0006 | LOSS 0.1967\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0005 / 0006 | LOSS 0.1952\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0006 / 0006 | LOSS 0.1941\n","VALID: EPOCH 0027 / 0100 | BATCH 0001 / 0001 | LOSS 0.2204\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0001 / 0006 | LOSS 0.1912\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0002 / 0006 | LOSS 0.1877\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0003 / 0006 | LOSS 0.1888\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0004 / 0006 | LOSS 0.1876\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0005 / 0006 | LOSS 0.1912\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0006 / 0006 | LOSS 0.1929\n","VALID: EPOCH 0028 / 0100 | BATCH 0001 / 0001 | LOSS 0.2138\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0001 / 0006 | LOSS 0.1953\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0002 / 0006 | LOSS 0.1910\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0003 / 0006 | LOSS 0.1920\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0004 / 0006 | LOSS 0.1923\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0005 / 0006 | LOSS 0.1913\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0006 / 0006 | LOSS 0.1897\n","VALID: EPOCH 0029 / 0100 | BATCH 0001 / 0001 | LOSS 0.2170\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0001 / 0006 | LOSS 0.1946\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0002 / 0006 | LOSS 0.2036\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0003 / 0006 | LOSS 0.1985\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0004 / 0006 | LOSS 0.1931\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0005 / 0006 | LOSS 0.1924\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0006 / 0006 | LOSS 0.1920\n","VALID: EPOCH 0030 / 0100 | BATCH 0001 / 0001 | LOSS 0.2183\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0001 / 0006 | LOSS 0.1932\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0002 / 0006 | LOSS 0.1895\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0003 / 0006 | LOSS 0.1893\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0004 / 0006 | LOSS 0.1835\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0005 / 0006 | LOSS 0.1876\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0006 / 0006 | LOSS 0.1864\n","VALID: EPOCH 0031 / 0100 | BATCH 0001 / 0001 | LOSS 0.2139\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0001 / 0006 | LOSS 0.2035\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0002 / 0006 | LOSS 0.1861\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0003 / 0006 | LOSS 0.1821\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0004 / 0006 | LOSS 0.1844\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0005 / 0006 | LOSS 0.1925\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0006 / 0006 | LOSS 0.1901\n","VALID: EPOCH 0032 / 0100 | BATCH 0001 / 0001 | LOSS 0.2112\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0001 / 0006 | LOSS 0.1820\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0002 / 0006 | LOSS 0.1890\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0003 / 0006 | LOSS 0.1913\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0004 / 0006 | LOSS 0.1903\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0005 / 0006 | LOSS 0.1913\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0006 / 0006 | LOSS 0.1904\n","VALID: EPOCH 0033 / 0100 | BATCH 0001 / 0001 | LOSS 0.2181\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0001 / 0006 | LOSS 0.1828\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0002 / 0006 | LOSS 0.1886\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0003 / 0006 | LOSS 0.1886\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0004 / 0006 | LOSS 0.1890\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0005 / 0006 | LOSS 0.1901\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0006 / 0006 | LOSS 0.1896\n","VALID: EPOCH 0034 / 0100 | BATCH 0001 / 0001 | LOSS 0.2114\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0001 / 0006 | LOSS 0.1853\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0002 / 0006 | LOSS 0.1806\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0003 / 0006 | LOSS 0.1779\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0004 / 0006 | LOSS 0.1828\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0005 / 0006 | LOSS 0.1828\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0006 / 0006 | LOSS 0.1834\n","VALID: EPOCH 0035 / 0100 | BATCH 0001 / 0001 | LOSS 0.2125\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0001 / 0006 | LOSS 0.1871\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0002 / 0006 | LOSS 0.1922\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0003 / 0006 | LOSS 0.1873\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0004 / 0006 | LOSS 0.1864\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0005 / 0006 | LOSS 0.1856\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0006 / 0006 | LOSS 0.1859\n","VALID: EPOCH 0036 / 0100 | BATCH 0001 / 0001 | LOSS 0.2112\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0001 / 0006 | LOSS 0.1909\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0002 / 0006 | LOSS 0.1857\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0003 / 0006 | LOSS 0.1875\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0004 / 0006 | LOSS 0.1864\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0005 / 0006 | LOSS 0.1855\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0006 / 0006 | LOSS 0.1819\n","VALID: EPOCH 0037 / 0100 | BATCH 0001 / 0001 | LOSS 0.2235\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0001 / 0006 | LOSS 0.2019\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0002 / 0006 | LOSS 0.1971\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0003 / 0006 | LOSS 0.1883\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0004 / 0006 | LOSS 0.1852\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0005 / 0006 | LOSS 0.1887\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0006 / 0006 | LOSS 0.1873\n","VALID: EPOCH 0038 / 0100 | BATCH 0001 / 0001 | LOSS 0.2072\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0001 / 0006 | LOSS 0.1845\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0002 / 0006 | LOSS 0.1790\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0003 / 0006 | LOSS 0.1861\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0004 / 0006 | LOSS 0.1802\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0005 / 0006 | LOSS 0.1837\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0006 / 0006 | LOSS 0.1842\n","VALID: EPOCH 0039 / 0100 | BATCH 0001 / 0001 | LOSS 0.2080\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0001 / 0006 | LOSS 0.1752\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0002 / 0006 | LOSS 0.1809\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0003 / 0006 | LOSS 0.1831\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0004 / 0006 | LOSS 0.1817\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0005 / 0006 | LOSS 0.1809\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0006 / 0006 | LOSS 0.1798\n","VALID: EPOCH 0040 / 0100 | BATCH 0001 / 0001 | LOSS 0.2207\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0001 / 0006 | LOSS 0.1793\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0002 / 0006 | LOSS 0.1881\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0003 / 0006 | LOSS 0.1837\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0004 / 0006 | LOSS 0.1843\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0005 / 0006 | LOSS 0.1818\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0006 / 0006 | LOSS 0.1793\n","VALID: EPOCH 0041 / 0100 | BATCH 0001 / 0001 | LOSS 0.2106\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0001 / 0006 | LOSS 0.1693\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0002 / 0006 | LOSS 0.1708\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0003 / 0006 | LOSS 0.1723\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0004 / 0006 | LOSS 0.1775\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0005 / 0006 | LOSS 0.1762\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0006 / 0006 | LOSS 0.1766\n","VALID: EPOCH 0042 / 0100 | BATCH 0001 / 0001 | LOSS 0.2085\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0001 / 0006 | LOSS 0.1699\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0002 / 0006 | LOSS 0.1768\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0003 / 0006 | LOSS 0.1765\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0004 / 0006 | LOSS 0.1801\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0005 / 0006 | LOSS 0.1779\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0006 / 0006 | LOSS 0.1765\n","VALID: EPOCH 0043 / 0100 | BATCH 0001 / 0001 | LOSS 0.2121\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0001 / 0006 | LOSS 0.1626\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0002 / 0006 | LOSS 0.1732\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0003 / 0006 | LOSS 0.1808\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0004 / 0006 | LOSS 0.1825\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0005 / 0006 | LOSS 0.1801\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0006 / 0006 | LOSS 0.1766\n","VALID: EPOCH 0044 / 0100 | BATCH 0001 / 0001 | LOSS 0.2020\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0001 / 0006 | LOSS 0.1688\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0002 / 0006 | LOSS 0.1660\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0003 / 0006 | LOSS 0.1663\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0004 / 0006 | LOSS 0.1726\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0005 / 0006 | LOSS 0.1789\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0006 / 0006 | LOSS 0.1760\n","VALID: EPOCH 0045 / 0100 | BATCH 0001 / 0001 | LOSS 0.2077\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0001 / 0006 | LOSS 0.1850\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0002 / 0006 | LOSS 0.1830\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0003 / 0006 | LOSS 0.1812\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0004 / 0006 | LOSS 0.1785\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0005 / 0006 | LOSS 0.1783\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0006 / 0006 | LOSS 0.1753\n","VALID: EPOCH 0046 / 0100 | BATCH 0001 / 0001 | LOSS 0.2076\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0001 / 0006 | LOSS 0.1812\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0002 / 0006 | LOSS 0.1783\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0003 / 0006 | LOSS 0.1768\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0004 / 0006 | LOSS 0.1789\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0005 / 0006 | LOSS 0.1777\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0006 / 0006 | LOSS 0.1763\n","VALID: EPOCH 0047 / 0100 | BATCH 0001 / 0001 | LOSS 0.2114\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0001 / 0006 | LOSS 0.1738\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0002 / 0006 | LOSS 0.1752\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0003 / 0006 | LOSS 0.1718\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0004 / 0006 | LOSS 0.1692\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0005 / 0006 | LOSS 0.1726\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0006 / 0006 | LOSS 0.1753\n","VALID: EPOCH 0048 / 0100 | BATCH 0001 / 0001 | LOSS 0.2087\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0001 / 0006 | LOSS 0.1794\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0002 / 0006 | LOSS 0.1769\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0003 / 0006 | LOSS 0.1788\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0004 / 0006 | LOSS 0.1749\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0005 / 0006 | LOSS 0.1739\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0006 / 0006 | LOSS 0.1728\n","VALID: EPOCH 0049 / 0100 | BATCH 0001 / 0001 | LOSS 0.2218\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0001 / 0006 | LOSS 0.1737\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0002 / 0006 | LOSS 0.1730\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0003 / 0006 | LOSS 0.1699\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0004 / 0006 | LOSS 0.1763\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0005 / 0006 | LOSS 0.1738\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0006 / 0006 | LOSS 0.1736\n","VALID: EPOCH 0050 / 0100 | BATCH 0001 / 0001 | LOSS 0.2252\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0001 / 0006 | LOSS 0.1720\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0002 / 0006 | LOSS 0.1746\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0003 / 0006 | LOSS 0.1748\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0004 / 0006 | LOSS 0.1748\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0005 / 0006 | LOSS 0.1746\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0006 / 0006 | LOSS 0.1755\n","VALID: EPOCH 0051 / 0100 | BATCH 0001 / 0001 | LOSS 0.2179\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0001 / 0006 | LOSS 0.1840\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0002 / 0006 | LOSS 0.1797\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0003 / 0006 | LOSS 0.1758\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0004 / 0006 | LOSS 0.1737\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0005 / 0006 | LOSS 0.1746\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0006 / 0006 | LOSS 0.1737\n","VALID: EPOCH 0052 / 0100 | BATCH 0001 / 0001 | LOSS 0.2007\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0001 / 0006 | LOSS 0.1811\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0002 / 0006 | LOSS 0.1798\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0003 / 0006 | LOSS 0.1757\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0004 / 0006 | LOSS 0.1773\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0005 / 0006 | LOSS 0.1750\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0006 / 0006 | LOSS 0.1729\n","VALID: EPOCH 0053 / 0100 | BATCH 0001 / 0001 | LOSS 0.2026\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0001 / 0006 | LOSS 0.1727\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0002 / 0006 | LOSS 0.1741\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0003 / 0006 | LOSS 0.1757\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0004 / 0006 | LOSS 0.1730\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0005 / 0006 | LOSS 0.1717\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0006 / 0006 | LOSS 0.1727\n","VALID: EPOCH 0054 / 0100 | BATCH 0001 / 0001 | LOSS 0.1974\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0001 / 0006 | LOSS 0.1793\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0002 / 0006 | LOSS 0.1704\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0003 / 0006 | LOSS 0.1738\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0004 / 0006 | LOSS 0.1710\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0005 / 0006 | LOSS 0.1692\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0006 / 0006 | LOSS 0.1690\n","VALID: EPOCH 0055 / 0100 | BATCH 0001 / 0001 | LOSS 0.2012\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0001 / 0006 | LOSS 0.1656\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0002 / 0006 | LOSS 0.1684\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0003 / 0006 | LOSS 0.1690\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0004 / 0006 | LOSS 0.1665\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0005 / 0006 | LOSS 0.1670\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0006 / 0006 | LOSS 0.1682\n","VALID: EPOCH 0056 / 0100 | BATCH 0001 / 0001 | LOSS 0.2049\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0001 / 0006 | LOSS 0.1658\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0002 / 0006 | LOSS 0.1665\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0003 / 0006 | LOSS 0.1712\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0004 / 0006 | LOSS 0.1676\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0005 / 0006 | LOSS 0.1713\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0006 / 0006 | LOSS 0.1694\n","VALID: EPOCH 0057 / 0100 | BATCH 0001 / 0001 | LOSS 0.2025\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0001 / 0006 | LOSS 0.1673\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0002 / 0006 | LOSS 0.1630\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0003 / 0006 | LOSS 0.1676\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0004 / 0006 | LOSS 0.1690\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0005 / 0006 | LOSS 0.1664\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0006 / 0006 | LOSS 0.1666\n","VALID: EPOCH 0058 / 0100 | BATCH 0001 / 0001 | LOSS 0.2277\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0001 / 0006 | LOSS 0.1950\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0002 / 0006 | LOSS 0.1844\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0003 / 0006 | LOSS 0.1738\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0004 / 0006 | LOSS 0.1701\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0005 / 0006 | LOSS 0.1677\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0006 / 0006 | LOSS 0.1696\n","VALID: EPOCH 0059 / 0100 | BATCH 0001 / 0001 | LOSS 0.2150\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0001 / 0006 | LOSS 0.1667\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0002 / 0006 | LOSS 0.1750\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0003 / 0006 | LOSS 0.1798\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0004 / 0006 | LOSS 0.1753\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0005 / 0006 | LOSS 0.1742\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0006 / 0006 | LOSS 0.1746\n","VALID: EPOCH 0060 / 0100 | BATCH 0001 / 0001 | LOSS 0.2228\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0001 / 0006 | LOSS 0.1677\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0002 / 0006 | LOSS 0.1786\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0003 / 0006 | LOSS 0.1751\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0004 / 0006 | LOSS 0.1733\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0005 / 0006 | LOSS 0.1723\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0006 / 0006 | LOSS 0.1708\n","VALID: EPOCH 0061 / 0100 | BATCH 0001 / 0001 | LOSS 0.2056\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0001 / 0006 | LOSS 0.1633\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0002 / 0006 | LOSS 0.1607\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0003 / 0006 | LOSS 0.1669\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0004 / 0006 | LOSS 0.1672\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0005 / 0006 | LOSS 0.1674\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0006 / 0006 | LOSS 0.1692\n","VALID: EPOCH 0062 / 0100 | BATCH 0001 / 0001 | LOSS 0.2088\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0001 / 0006 | LOSS 0.1632\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0002 / 0006 | LOSS 0.1614\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0003 / 0006 | LOSS 0.1648\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0004 / 0006 | LOSS 0.1624\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0005 / 0006 | LOSS 0.1632\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0006 / 0006 | LOSS 0.1650\n","VALID: EPOCH 0063 / 0100 | BATCH 0001 / 0001 | LOSS 0.2044\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0001 / 0006 | LOSS 0.1619\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0002 / 0006 | LOSS 0.1632\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0003 / 0006 | LOSS 0.1638\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0004 / 0006 | LOSS 0.1628\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0005 / 0006 | LOSS 0.1631\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0006 / 0006 | LOSS 0.1628\n","VALID: EPOCH 0064 / 0100 | BATCH 0001 / 0001 | LOSS 0.2224\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0001 / 0006 | LOSS 0.1619\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0002 / 0006 | LOSS 0.1587\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0003 / 0006 | LOSS 0.1663\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0004 / 0006 | LOSS 0.1640\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0005 / 0006 | LOSS 0.1633\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0006 / 0006 | LOSS 0.1639\n","VALID: EPOCH 0065 / 0100 | BATCH 0001 / 0001 | LOSS 0.2029\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0001 / 0006 | LOSS 0.1625\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0002 / 0006 | LOSS 0.1624\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0003 / 0006 | LOSS 0.1663\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0004 / 0006 | LOSS 0.1685\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0005 / 0006 | LOSS 0.1646\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0006 / 0006 | LOSS 0.1632\n","VALID: EPOCH 0066 / 0100 | BATCH 0001 / 0001 | LOSS 0.2013\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0001 / 0006 | LOSS 0.1676\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0002 / 0006 | LOSS 0.1599\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0003 / 0006 | LOSS 0.1632\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0004 / 0006 | LOSS 0.1601\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0005 / 0006 | LOSS 0.1572\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0006 / 0006 | LOSS 0.1597\n","VALID: EPOCH 0067 / 0100 | BATCH 0001 / 0001 | LOSS 0.2020\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0001 / 0006 | LOSS 0.1541\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0002 / 0006 | LOSS 0.1502\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0003 / 0006 | LOSS 0.1535\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0004 / 0006 | LOSS 0.1536\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0005 / 0006 | LOSS 0.1562\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0006 / 0006 | LOSS 0.1613\n","VALID: EPOCH 0068 / 0100 | BATCH 0001 / 0001 | LOSS 0.2050\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0001 / 0006 | LOSS 0.1669\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0002 / 0006 | LOSS 0.1615\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0003 / 0006 | LOSS 0.1634\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0004 / 0006 | LOSS 0.1629\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0005 / 0006 | LOSS 0.1629\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0006 / 0006 | LOSS 0.1659\n","VALID: EPOCH 0069 / 0100 | BATCH 0001 / 0001 | LOSS 0.2039\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0001 / 0006 | LOSS 0.1787\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0002 / 0006 | LOSS 0.1741\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0003 / 0006 | LOSS 0.1716\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0004 / 0006 | LOSS 0.1710\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0005 / 0006 | LOSS 0.1692\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0006 / 0006 | LOSS 0.1663\n","VALID: EPOCH 0070 / 0100 | BATCH 0001 / 0001 | LOSS 0.2030\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0001 / 0006 | LOSS 0.1628\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0002 / 0006 | LOSS 0.1546\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0003 / 0006 | LOSS 0.1535\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0004 / 0006 | LOSS 0.1642\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0005 / 0006 | LOSS 0.1652\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0006 / 0006 | LOSS 0.1665\n","VALID: EPOCH 0071 / 0100 | BATCH 0001 / 0001 | LOSS 0.2073\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0001 / 0006 | LOSS 0.1583\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0002 / 0006 | LOSS 0.1590\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0003 / 0006 | LOSS 0.1605\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0004 / 0006 | LOSS 0.1691\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0005 / 0006 | LOSS 0.1654\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0006 / 0006 | LOSS 0.1663\n","VALID: EPOCH 0072 / 0100 | BATCH 0001 / 0001 | LOSS 0.2078\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0001 / 0006 | LOSS 0.1678\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0002 / 0006 | LOSS 0.1611\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0003 / 0006 | LOSS 0.1621\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0004 / 0006 | LOSS 0.1636\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0005 / 0006 | LOSS 0.1628\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0006 / 0006 | LOSS 0.1624\n","VALID: EPOCH 0073 / 0100 | BATCH 0001 / 0001 | LOSS 0.2093\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0001 / 0006 | LOSS 0.1705\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0002 / 0006 | LOSS 0.1621\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0003 / 0006 | LOSS 0.1667\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0004 / 0006 | LOSS 0.1629\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0005 / 0006 | LOSS 0.1624\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0006 / 0006 | LOSS 0.1612\n","VALID: EPOCH 0074 / 0100 | BATCH 0001 / 0001 | LOSS 0.2126\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0001 / 0006 | LOSS 0.1558\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0002 / 0006 | LOSS 0.1579\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0003 / 0006 | LOSS 0.1617\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0004 / 0006 | LOSS 0.1604\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0005 / 0006 | LOSS 0.1584\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0006 / 0006 | LOSS 0.1597\n","VALID: EPOCH 0075 / 0100 | BATCH 0001 / 0001 | LOSS 0.1984\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0001 / 0006 | LOSS 0.1716\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0002 / 0006 | LOSS 0.1695\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0003 / 0006 | LOSS 0.1633\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0004 / 0006 | LOSS 0.1637\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0005 / 0006 | LOSS 0.1617\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0006 / 0006 | LOSS 0.1591\n","VALID: EPOCH 0076 / 0100 | BATCH 0001 / 0001 | LOSS 0.2067\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0001 / 0006 | LOSS 0.1649\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0002 / 0006 | LOSS 0.1642\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0003 / 0006 | LOSS 0.1683\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0004 / 0006 | LOSS 0.1673\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0005 / 0006 | LOSS 0.1641\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0006 / 0006 | LOSS 0.1643\n","VALID: EPOCH 0077 / 0100 | BATCH 0001 / 0001 | LOSS 0.2042\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0001 / 0006 | LOSS 0.1584\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0002 / 0006 | LOSS 0.1610\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0003 / 0006 | LOSS 0.1598\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0004 / 0006 | LOSS 0.1589\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0005 / 0006 | LOSS 0.1575\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0006 / 0006 | LOSS 0.1593\n","VALID: EPOCH 0078 / 0100 | BATCH 0001 / 0001 | LOSS 0.1970\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0001 / 0006 | LOSS 0.1534\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0002 / 0006 | LOSS 0.1510\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0003 / 0006 | LOSS 0.1562\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0004 / 0006 | LOSS 0.1550\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0005 / 0006 | LOSS 0.1574\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0006 / 0006 | LOSS 0.1576\n","VALID: EPOCH 0079 / 0100 | BATCH 0001 / 0001 | LOSS 0.2213\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0001 / 0006 | LOSS 0.1667\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0002 / 0006 | LOSS 0.1629\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0003 / 0006 | LOSS 0.1591\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0004 / 0006 | LOSS 0.1592\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0005 / 0006 | LOSS 0.1570\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0006 / 0006 | LOSS 0.1559\n","VALID: EPOCH 0080 / 0100 | BATCH 0001 / 0001 | LOSS 0.2069\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0001 / 0006 | LOSS 0.1662\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0002 / 0006 | LOSS 0.1603\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0003 / 0006 | LOSS 0.1555\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0004 / 0006 | LOSS 0.1556\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0005 / 0006 | LOSS 0.1555\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0006 / 0006 | LOSS 0.1558\n","VALID: EPOCH 0081 / 0100 | BATCH 0001 / 0001 | LOSS 0.2046\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0001 / 0006 | LOSS 0.1550\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0002 / 0006 | LOSS 0.1504\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0003 / 0006 | LOSS 0.1517\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0004 / 0006 | LOSS 0.1523\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0005 / 0006 | LOSS 0.1519\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0006 / 0006 | LOSS 0.1525\n","VALID: EPOCH 0082 / 0100 | BATCH 0001 / 0001 | LOSS 0.1967\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0001 / 0006 | LOSS 0.1417\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0002 / 0006 | LOSS 0.1451\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0003 / 0006 | LOSS 0.1511\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0004 / 0006 | LOSS 0.1482\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0005 / 0006 | LOSS 0.1480\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0006 / 0006 | LOSS 0.1523\n","VALID: EPOCH 0083 / 0100 | BATCH 0001 / 0001 | LOSS 0.2035\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0001 / 0006 | LOSS 0.1435\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0002 / 0006 | LOSS 0.1468\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0003 / 0006 | LOSS 0.1497\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0004 / 0006 | LOSS 0.1513\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0005 / 0006 | LOSS 0.1503\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0006 / 0006 | LOSS 0.1501\n","VALID: EPOCH 0084 / 0100 | BATCH 0001 / 0001 | LOSS 0.2066\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0001 / 0006 | LOSS 0.1663\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0002 / 0006 | LOSS 0.1606\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0003 / 0006 | LOSS 0.1577\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0004 / 0006 | LOSS 0.1528\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0005 / 0006 | LOSS 0.1515\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0006 / 0006 | LOSS 0.1524\n","VALID: EPOCH 0085 / 0100 | BATCH 0001 / 0001 | LOSS 0.2051\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0001 / 0006 | LOSS 0.1562\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0002 / 0006 | LOSS 0.1628\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0003 / 0006 | LOSS 0.1564\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0004 / 0006 | LOSS 0.1524\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0005 / 0006 | LOSS 0.1526\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0006 / 0006 | LOSS 0.1527\n","VALID: EPOCH 0086 / 0100 | BATCH 0001 / 0001 | LOSS 0.2157\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0001 / 0006 | LOSS 0.1560\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0002 / 0006 | LOSS 0.1595\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0003 / 0006 | LOSS 0.1579\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0004 / 0006 | LOSS 0.1551\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0005 / 0006 | LOSS 0.1526\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0006 / 0006 | LOSS 0.1510\n","VALID: EPOCH 0087 / 0100 | BATCH 0001 / 0001 | LOSS 0.2082\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0001 / 0006 | LOSS 0.1452\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0002 / 0006 | LOSS 0.1447\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0003 / 0006 | LOSS 0.1507\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0004 / 0006 | LOSS 0.1501\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0005 / 0006 | LOSS 0.1498\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0006 / 0006 | LOSS 0.1537\n","VALID: EPOCH 0088 / 0100 | BATCH 0001 / 0001 | LOSS 0.2218\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0001 / 0006 | LOSS 0.1586\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0002 / 0006 | LOSS 0.1637\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0003 / 0006 | LOSS 0.1615\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0004 / 0006 | LOSS 0.1593\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0005 / 0006 | LOSS 0.1566\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0006 / 0006 | LOSS 0.1546\n","VALID: EPOCH 0089 / 0100 | BATCH 0001 / 0001 | LOSS 0.2204\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0001 / 0006 | LOSS 0.1580\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0002 / 0006 | LOSS 0.1587\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0003 / 0006 | LOSS 0.1567\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0004 / 0006 | LOSS 0.1528\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0005 / 0006 | LOSS 0.1511\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0006 / 0006 | LOSS 0.1529\n","VALID: EPOCH 0090 / 0100 | BATCH 0001 / 0001 | LOSS 0.2081\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0001 / 0006 | LOSS 0.1491\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0002 / 0006 | LOSS 0.1513\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0003 / 0006 | LOSS 0.1528\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0004 / 0006 | LOSS 0.1514\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0005 / 0006 | LOSS 0.1540\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0006 / 0006 | LOSS 0.1529\n","VALID: EPOCH 0091 / 0100 | BATCH 0001 / 0001 | LOSS 0.2147\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0001 / 0006 | LOSS 0.1463\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0002 / 0006 | LOSS 0.1531\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0003 / 0006 | LOSS 0.1522\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0004 / 0006 | LOSS 0.1532\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0005 / 0006 | LOSS 0.1548\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0006 / 0006 | LOSS 0.1537\n","VALID: EPOCH 0092 / 0100 | BATCH 0001 / 0001 | LOSS 0.2123\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0001 / 0006 | LOSS 0.1551\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0002 / 0006 | LOSS 0.1543\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0003 / 0006 | LOSS 0.1467\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0004 / 0006 | LOSS 0.1478\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0005 / 0006 | LOSS 0.1563\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0006 / 0006 | LOSS 0.1554\n","VALID: EPOCH 0093 / 0100 | BATCH 0001 / 0001 | LOSS 0.2403\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0001 / 0006 | LOSS 0.1618\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0002 / 0006 | LOSS 0.1572\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0003 / 0006 | LOSS 0.1519\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0004 / 0006 | LOSS 0.1492\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0005 / 0006 | LOSS 0.1550\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0006 / 0006 | LOSS 0.1551\n","VALID: EPOCH 0094 / 0100 | BATCH 0001 / 0001 | LOSS 0.2085\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0001 / 0006 | LOSS 0.1477\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0002 / 0006 | LOSS 0.1602\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0003 / 0006 | LOSS 0.1553\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0004 / 0006 | LOSS 0.1535\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0005 / 0006 | LOSS 0.1512\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0006 / 0006 | LOSS 0.1524\n","VALID: EPOCH 0095 / 0100 | BATCH 0001 / 0001 | LOSS 0.2200\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0001 / 0006 | LOSS 0.1509\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0002 / 0006 | LOSS 0.1515\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0003 / 0006 | LOSS 0.1482\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0004 / 0006 | LOSS 0.1495\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0005 / 0006 | LOSS 0.1468\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0006 / 0006 | LOSS 0.1485\n","VALID: EPOCH 0096 / 0100 | BATCH 0001 / 0001 | LOSS 0.2115\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0001 / 0006 | LOSS 0.1426\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0002 / 0006 | LOSS 0.1457\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0003 / 0006 | LOSS 0.1468\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0004 / 0006 | LOSS 0.1462\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0005 / 0006 | LOSS 0.1462\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0006 / 0006 | LOSS 0.1470\n","VALID: EPOCH 0097 / 0100 | BATCH 0001 / 0001 | LOSS 0.2019\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0001 / 0006 | LOSS 0.1343\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0002 / 0006 | LOSS 0.1462\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0003 / 0006 | LOSS 0.1451\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0004 / 0006 | LOSS 0.1460\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0005 / 0006 | LOSS 0.1476\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0006 / 0006 | LOSS 0.1482\n","VALID: EPOCH 0098 / 0100 | BATCH 0001 / 0001 | LOSS 0.2166\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0001 / 0006 | LOSS 0.1553\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0002 / 0006 | LOSS 0.1515\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0003 / 0006 | LOSS 0.1457\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0004 / 0006 | LOSS 0.1470\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0005 / 0006 | LOSS 0.1482\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0006 / 0006 | LOSS 0.1494\n","VALID: EPOCH 0099 / 0100 | BATCH 0001 / 0001 | LOSS 0.2087\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0001 / 0006 | LOSS 0.1427\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0002 / 0006 | LOSS 0.1487\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0003 / 0006 | LOSS 0.1478\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0004 / 0006 | LOSS 0.1462\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0005 / 0006 | LOSS 0.1441\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0006 / 0006 | LOSS 0.1454\n","VALID: EPOCH 0100 / 0100 | BATCH 0001 / 0001 | LOSS 0.2033\n"]}],"source":["!python3 '/home/work/deep/xai/3주차과제/train.py'"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"colab_type":"code","executionInfo":{"elapsed":7969,"status":"ok","timestamp":1584314467883,"user":{"displayName":"한요섭","photoUrl":"https://lh5.googleusercontent.com/-YFCq7U3ZRkA/AAAAAAAAAAI/AAAAAAAARHc/gXM5cCKzru0/s64/photo.jpg","userId":"13321053678363808287"},"user_tz":360},"id":"udJh_9lGeDrF","outputId":"642860fa-7b23-4e51-cc53-cec3538bb1a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-03-25 06:33:31.905816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-25 06:33:31.905891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-25 06:33:31.906759: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-25 06:33:31.912339: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-03-25 06:33:32.812545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","TEST: BATCH 0001 / 0001 | LOSS 0.1870\n","AVERAGE TEST: BATCH 0001 / 0001 | LOSS 0.1870\n"]}],"source":["!python3 '/home/work/deep/xai/3주차과제/eval.py'"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":12442,"status":"ok","timestamp":1584852126774,"user":{"displayName":"한요섭","photoUrl":"https://lh5.googleusercontent.com/-YFCq7U3ZRkA/AAAAAAAAAAI/AAAAAAAARHc/gXM5cCKzru0/s64/photo.jpg","userId":"13321053678363808287"},"user_tz":360},"id":"pz7ioSlzeVeN","outputId":"c569fe40-0776-4c5f-9097-c1fece9a14f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-03-25 07:01:05.693320: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-25 07:01:05.693389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-25 07:01:05.694232: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-25 07:01:05.699752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-03-25 07:01:06.597393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","learning rate: 1.0000e-01\n","batch size: 4\n","number of epoch: 512\n","data dir: /home/work/deep/xai/3주차과제/datasets\n","ckpt dir: /home/work/deep/xai/3주차과제/checkpoint_v2\n","log dir: /home/work/deep/xai/3주차과제/log_v2\n","result dir: /home/work/deep/xai/3주차과제/result_v2\n","mode: test\n","TEST: BATCH 0001 / 0001 | LOSS 0.6773\n","AVERAGE TEST: BATCH 0001 / 0001 | LOSS 0.6773\n"]}],"source":["!python3 \"/home/work/deep/xai/3주차과제/train.py\" \\\n","--lr 1e-1 --batch_size 4 --num_epoch 512 \\\n","--data_dir \"/home/work/deep/xai/3주차과제/datasets\" \\\n","--ckpt_dir \"/home/work/deep/xai/3주차과제/checkpoint_v2\" \\\n","--log_dir \"/home/work/deep/xai/3주차과제/log_v2\" \\\n","--result_dir \"/home/work/deep/xai/3주차과제/result_v2\" \\\n","--mode \"test\" \\\n","--train_continue \"off\""]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{},"colab_type":"code","id":"CVoZZbqBbGcR"},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-03-25 07:02:26.312829: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-25 07:02:26.312899: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-25 07:02:26.313734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-25 07:02:26.319244: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-03-25 07:02:27.214225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","learning rate: 1.0000e-02\n","batch size: 8\n","number of epoch: 256\n","data dir: /home/work/deep/xai/3주차과제/datasets\n","ckpt dir: /home/work/deep/xai/3주차과제/checkpoint_v2\n","log dir: /home/work/deep/xai/3주차과제/log_v2\n","result dir: /home/work/deep/xai/3주차과제/result_v2\n","mode: test\n","TEST: BATCH 0001 / 0001 | LOSS 0.6640\n","AVERAGE TEST: BATCH 0001 / 0001 | LOSS 0.6640\n"]}],"source":["!python3 \"/home/work/deep/xai/3주차과제/train.py\" \\\n","--lr 1e-2 --batch_size 8 --num_epoch 256 \\\n","--data_dir \"/home/work/deep/xai/3주차과제/datasets\" \\\n","--ckpt_dir \"/home/work/deep/xai/3주차과제/checkpoint_v2\" \\\n","--log_dir \"/home/work/deep/xai/3주차과제/log_v2\" \\\n","--result_dir \"/home/work/deep/xai/3주차과제/result_v2\" \\\n","--mode \"test\" \\\n","--train_continue \"off\""]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-03-25 08:22:56.300812: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-25 08:22:56.300882: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-25 08:22:56.301851: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-25 08:22:56.307717: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-03-25 08:22:57.210792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","learning rate: 1.0000e-03\n","batch size: 2\n","number of epoch: 2048\n","data dir: /home/work/deep/xai/3주차과제/datasets\n","ckpt dir: /home/work/deep/xai/3주차과제/checkpoint_v2\n","log dir: /home/work/deep/xai/3주차과제/log_v2\n","result dir: /home/work/deep/xai/3주차과제/result_v2\n","mode: test\n","TEST: BATCH 0001 / 0002 | LOSS 0.6587\n","TEST: BATCH 0002 / 0002 | LOSS 0.6575\n","AVERAGE TEST: BATCH 0002 / 0002 | LOSS 0.6575\n"]}],"source":["!python3 \"/home/work/deep/xai/3주차과제/train.py\" \\\n","--lr 1e-3 --batch_size 2 --num_epoch 2048 \\\n","--data_dir \"/home/work/deep/xai/3주차과제/datasets\" \\\n","--ckpt_dir \"/home/work/deep/xai/3주차과제/checkpoint_v2\" \\\n","--log_dir \"/home/work/deep/xai/3주차과제/log_v2\" \\\n","--result_dir \"/home/work/deep/xai/3주차과제/result_v2\" \\\n","--mode \"test\" \\\n","--train_continue \"off\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOhn0PSXfBvXCiYxsnaoSU4","collapsed_sections":[],"mount_file_id":"1cFB-vCtcEHFjk1t7cXHJHg0Nog4LiOMZ","name":"run_unet.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
